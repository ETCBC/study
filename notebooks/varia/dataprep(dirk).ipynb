{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Towards a valency dictionary"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A valency dictionary is a kind of flow chart by which you can separate out verb meanings based on the number and nature of the complements they take. We want to be able to automatically match contexts against flow charts.\n",
      "\n",
      "The work in this notebook consists of two parts: applying corrections to the data and implementing flow charts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import sys\n",
      "import collections\n",
      "import shutil\n",
      "\n",
      "import pandas\n",
      "from IPython.display import display\n",
      "pandas.set_option('display.notebook_repr_html', True)\n",
      "\n",
      "from laf.fabric import LafFabric\n",
      "fabric = LafFabric()\n",
      "from etcbc.preprocess import prepare\n",
      "from etcbc.annotating import GenForm\n",
      "\n",
      "fabric = LafFabric()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s This is LAF-Fabric 4.0.3\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s This is LAF-Fabric 4.0.3\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s END\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fabric.load('bhs3.txt.hdr', '--', 'dataprep',\n",
      "        {\n",
      "            \"xmlids\": {\n",
      "                \"node\": False,\n",
      "                \"edge\": False,\n",
      "            },\n",
      "            \"features\": {\n",
      "                \"shebanq\": {\n",
      "                    \"node\": [\n",
      "                        \"db.otype,monads\",\n",
      "                        \"ft.text,surface_consonants\",\n",
      "                        \"ft.phrase_function,locative\",\n",
      "                        \"sft.book,chapter,verse_label,verse\",\n",
      "                    ],\n",
      "                    \"edge\": [\n",
      "                    ],\n",
      "                },\n",
      "            },\n",
      "            'prepare': prepare,\n",
      "        },\n",
      "        compile_main=False, compile_annox=False,\n",
      "        verbose='DETAIL',\n",
      "    )\n",
      "\n",
      "exec(fabric.localnames.format(var='fabric'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s LOADING API: please wait ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING m: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING a: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: load main: G.node_anchor_min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.11s DETAIL: load main: G.node_anchor_max\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.21s DETAIL: load main: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.31s DETAIL: load main: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.80s DETAIL: load main: G.edges_from\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.88s DETAIL: load main: G.edges_to\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.97s DETAIL: load main: F.shebanq_db_otype [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.64s DETAIL: load main: F.shebanq_db_monads [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.48s DETAIL: load main: F.shebanq_ft_text [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.82s DETAIL: load main: F.shebanq_ft_surface_consonants [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.04s DETAIL: load main: F.shebanq_ft_phrase_function [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.16s DETAIL: load main: F.shebanq_ft_locative [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.36s DETAIL: load main: F.shebanq_sft_book [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.38s DETAIL: load main: F.shebanq_sft_chapter [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.40s DETAIL: load main: F.shebanq_sft_verse_label [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.42s DETAIL: load main: F.shebanq_sft_verse [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.43s LOGFILE=/Users/judith/laf-fabric-data/etcbc-bhs3/tasks/bhs3.txt.hdr/dataprep/__log__dataprep.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.43s DETAIL: prep prep: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.54s DETAIL: prep prep: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.06s INFO: DATA LOADED FROM SOURCE bhs3.txt.hdr AND ANNOX -- FOR TASK dataprep\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "form = GenForm(API, \"dataprep_corrections\", {\n",
      "    'target_types': [\n",
      "        'phrase',\n",
      "    ],\n",
      "    'show_features': {\n",
      "        'shebanq': {\n",
      "            'node': [\n",
      "                \"ft.phrase_function\",\n",
      "            ],\n",
      "        },\n",
      "    },\n",
      "    'new_features': {\n",
      "        'judith': {\n",
      "            'node': [\n",
      "                \"part.new_labels\",\n",
      "            ],\n",
      "        },\n",
      "    },\n",
      "})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "__init__() missing 1 required positional argument: 'config'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-51-9284fc23da04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         'judith': {\n\u001b[1;32m     14\u001b[0m             'node': [\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;34m\"part.new_labels\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             ],\n\u001b[1;32m     17\u001b[0m         },\n",
        "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'config'"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Applying corrections on the parsing analysis of the data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Janet has files with nearly formal corrections to the parsing analysis that has been coded into the BHS3 version of the Hebrew Text Database.\n",
      "\n",
      "We are going to translate those instructions into new LAF annotations: an annox package.\n",
      "\n",
      "# Specification of steps ahead\n",
      "\n",
      "1. In the corrections file, find out which of the phrases before :CHANGESTO: has to receive a modified phrase function.\n",
      "   This is the target phrase.\n",
      "2. Match the target phrase with a phrase occurrence in the indicated passage in the database.\n",
      "3. Produce an annoting input file, tab separated, 2 columns: the oid of the target phrase, the modified phrase_function.\n",
      "   The name of this column should be shebanq:ft.phrase_function\n",
      "4. Use the module etcbc.annotating (and the method make_form to produce a file of annotations\n",
      "5. put the file of annotations in a new annox (in the annotations directory)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Modeling the correction instructions\n",
      "\n",
      "We model the information in each rule in a datastructure, containing all the bits needed for the rest of the tasks in a readily available form.\n",
      "\n",
      "Have a look at the file with corrections, we show just one line here.\n",
      "We apply some consistency in book names and parsing labels between the ways they are represented in the corrections file and in the database."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "handle = open('CorrectionsFJM.txt', encoding=\"utf8\")\n",
      "lines = [line for line in handle]\n",
      "handle.close()\n",
      "lines[3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "'JES 41,19\\t[>FJM <Pr>]\\t[B <RBH <Lo>]\\t[BRWC TDHR W T>CWR <Ob>]\\t[JXDW <Mo>]\\t:CHANGESTO:\\t[B <RBH <Co>]\\n'"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "code_def = {\n",
      " '..': '..',\n",
      " 'Aj': 'Adju',\n",
      " 'Cj': 'Conj',\n",
      " 'Co': 'Cmpl',\n",
      " 'Lo': 'Loca',\n",
      " 'Mo': 'Modi',\n",
      " 'Ob': 'Objc',\n",
      " 'PO': 'PreO',\n",
      " 'Pr': 'Pred',\n",
      " 'Ps': 'PreS',\n",
      " 'Su': 'Subj',\n",
      " 'Ti': 'Time',\n",
      " 'sc': 'Supp',\n",
      "}\n",
      "\n",
      "book_def = {\n",
      "    'GEN': 'Genesis',\n",
      "    'EXO': 'Exodus',\n",
      "    'LEV': 'Leviticus',\n",
      "    'NUM': 'Numbers',\n",
      "    'DEU': 'Deuteronomy',\n",
      "    'JOS': 'Joshua',\n",
      "    'JUD': 'Judges',\n",
      "    'ISAM': 'I_Samuel',\n",
      "    'IISA': 'II_Samuel',\n",
      "    'IKON': 'I_Kings',\n",
      "    'IIKON': 'II_Kings',\n",
      "    'JES': 'Isaiah',\n",
      "    'JER': 'Jeremiah',\n",
      "    'EZE': 'Ezekiel',\n",
      "    'HOS': 'Hosea',\n",
      "    'JOE': 'Joel',\n",
      "    'AMO': 'Amos',\n",
      "    'OBA': 'Obadiah',\n",
      "    'JON': 'Jonah',\n",
      "    'MICH': 'Micah',\n",
      "    'NAH': 'Nahum',\n",
      "    'HAB': 'Habakkuk',\n",
      "    'ZEP': 'Zephaniah',\n",
      "    'HAG': 'Haggai',\n",
      "    'ZEC': 'Zechariah',\n",
      "    'MAL': 'Malachi',\n",
      "    'PS': 'Psalms',\n",
      "    'IOB': 'Job',\n",
      "    'PRO': 'Proverbs',\n",
      "    'RUT': 'Ruth',\n",
      "    'CAN': 'Canticles',\n",
      "    'ECC': 'Ecclesiastes',\n",
      "    'LAM': 'Lamentations',\n",
      "    'EST': 'Esther',\n",
      "    'DAN': 'Daniel',\n",
      "    'EZR': 'Ezra',\n",
      "    'NEH': 'Nehemiah',\n",
      "    'I_c': 'I_Chronicles',\n",
      "    'Ii_': 'II_Chronicles',    \n",
      "}\n",
      "\n",
      "def compile_rules(lines):\n",
      "    rules = []\n",
      "    for line in lines:\n",
      "        line = line.strip()\n",
      "        raw_all_fields = line.split('\\t')\n",
      "        passage = raw_all_fields[0]\n",
      "        p_comp = re.findall('^([A-Z]+)\\s*0*([0-9]+)\\s*,\\s*0*([0-9]+)\\s*$', passage)[0]\n",
      "        raw_fields = raw_all_fields[1:-2] + [raw_all_fields[-1]]\n",
      "        string_fields = [f.strip('[]') for f in raw_fields]\n",
      "        fields = [f.rsplit('<', 1) for f in string_fields]\n",
      "        new_fields = [(re.sub('\\s*\\+', '', f[0].rstrip()), code_def[f[1].rstrip('>')]) for f in fields]\n",
      "        rules.append([(book_def[p_comp[0]], p_comp[1], p_comp[2]), new_fields[-1], tuple(new_fields[0:-1])])\n",
      "    \n",
      "    for (i, rule) in enumerate(rules):\n",
      "        (passage, target, context) = rule\n",
      "        (target_words, target_code) = target\n",
      "        hits = []\n",
      "        the_hit = -1\n",
      "        for (n, cw) in enumerate(context):\n",
      "            if cw[0] == target_words: hits.append(n)\n",
      "        if not hits:\n",
      "            msg(\"WARNING: Rule {} [{}]: Nothing fits {} in context {}\".format(i+1, passage, target_words, context))\n",
      "        elif len(hits) > 1:\n",
      "            msg(\"WARNING: Rule {} [{}]: More than one phrase fit {} in context {}\".format(i+1, passage, target_words, context))\n",
      "            the_hit = hits[-1]\n",
      "        else: the_hit = hits[-1]\n",
      "        rule.append(the_hit)\n",
      "    return rules"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Retrieving passages\n",
      "\n",
      "We examine all the rules, collect the passages that we have to address, and collect data from them from the database."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_passage(passages):\n",
      "    cur_book = None\n",
      "    cur_chapter = None\n",
      "    cur_passage = None\n",
      "    cur_phrase = [None, []]\n",
      "    verse_phrases = []\n",
      "    \n",
      "    passages_dict = {}\n",
      "    inpassage = False\n",
      "    msg('Look for passages')\n",
      "    for n in NN():\n",
      "        otype = F.otype.v(n)\n",
      "        if otype == 'book':\n",
      "            cur_book = F.book.v(n)\n",
      "        elif otype == 'chapter':\n",
      "            cur_chapter = F.chapter.v(n)\n",
      "        elif otype == 'verse':\n",
      "            cur_verse = F.verse.v(n)\n",
      "            if cur_phrase[0] != None: verse_phrases.append(cur_phrase)\n",
      "            cur_phrase = [None, []]\n",
      "            if verse_phrases:\n",
      "                passages_dict[cur_passage] = {\n",
      "                    'elems': [\n",
      "                        (' '.join([F.surface_consonants.v(y) for y in x[1]]), F.phrase_function.v(x[0])) for x in verse_phrases\n",
      "                    ],\n",
      "                    'text': [\n",
      "                        [(F.text.v(y), F.monads.v(y)) for y in x[1]] for x in verse_phrases\n",
      "                    ],\n",
      "                }\n",
      "            verse_phrases = []\n",
      "            if (cur_book, cur_chapter, cur_verse) in passages: \n",
      "                cur_passage = (cur_book, cur_chapter, cur_verse)\n",
      "                inpassage = True\n",
      "                cur_phrase = [None, []]\n",
      "                verse_phrases = []\n",
      "            else: inpassage = False\n",
      "        elif inpassage:\n",
      "            if otype == 'word': \n",
      "                if F.surface_consonants.v(n) != '.': cur_phrase[1].append(n)   \n",
      "            elif otype == 'phrase':\n",
      "                if cur_phrase[0] != None: verse_phrases.append(cur_phrase)\n",
      "                cur_phrase = [n, []]\n",
      "    if cur_phrase[0] != None: verse_phrases.append(cur_phrase)\n",
      "    if verse_phrases:\n",
      "        passages_dict[cur_passage] = {\n",
      "            'elems': [\n",
      "                (' '.join([F.surface_consonants.v(y) for y in x[1]]), F.phrase_function.v(x[0])) for x in verse_phrases\n",
      "            ],\n",
      "            'text': [\n",
      "                [(F.text.v(y), F.monads.v(y)) for y in x[1]] for x in verse_phrases\n",
      "            ],\n",
      "        }\n",
      "    msg(\"End walk\")\n",
      "    return passages_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "rules = compile_rules(lines)\n",
      "show = 11\n",
      "rule = rules[show]\n",
      "(passage, target, context, offset) = rule\n",
      "verse = get_passage({passage})[passage]\n",
      "print(\"RULE {}=\\n{}\\nVERSE ELEMS:{}\\nVERSE TEXT={}\".format(show, rule, verse['elems'], verse['text']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    50s Look for passages\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    51s End walk\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RULE 11=\n",
        "[('II_Samuel', '14', '7'), ('L >JCJ', 'Adju'), (('L BLTJ FWM', 'Pred'), ('L >JCJ', 'Supp'), ('CM W C>RJT', 'Objc'), ('<L PNJ H >DMH', 'Cmpl')), 1]\n",
        "VERSE ELEMS:[('W', 'Conj'), ('HNH', 'Intj'), ('QMH', 'Pred'), ('KL H MCPXH', 'Subj'), ('<L CPXTK', 'Cmpl'), ('W', 'Conj'), ('J>MRW', 'Pred'), ('TNJ', 'Pred'), ('>T MKH >XJW', 'Objc'), ('W', 'Conj'), ('NMTHW', 'PreO'), ('B NPC >XJW', 'Adju'), ('>CR', 'Rela'), ('HRG', 'Pred'), ('W', 'Conj'), ('NCMJDH', 'Pred'), ('GM', 'Modi'), ('>T H JWRC', 'Objc'), ('W', 'Conj'), ('KBW', 'Pred'), ('>T GXLTJ', 'Objc'), ('>CR', 'Rela'), ('NC>RH', 'Pred'), ('L BLTJ', 'Conj'), ('FWM', 'Pred'), ('L >JCJ', 'Supp'), ('CM W C>RJT', 'Objc'), ('<L PNJ H >DMH', 'Cmpl')]\n",
        "VERSE TEXT=[[('\u05d5\u05b0', '168779')], [('\u05d4\u05b4\u05e0\u05b5\u05bc\u05d4\u05a9', '168780')], [('\u05e7\u05b8\u05a8\u05de\u05b8\u05d4', '168781')], [('\u05db\u05b8\u05bd\u05dc', '168782'), ('\u05d4\u05b7', '168783'), ('\u05de\u05b4\u05bc\u05e9\u05b0\u05c1\u05e4\u05b8\u05bc\u05d7\u05b8\u059c\u05d4', '168784')], [('\u05e2\u05b7\u05dc', '168785'), ('\u05e9\u05b4\u05c1\u05e4\u05b0\u05d7\u05b8\u05ea\u05b6\u0597\u05da\u05b8', '168786')], [('\u05d5\u05b7', '168787')], [('\u05d9\u05b9\u05bc\u05bd\u05d0\u05de\u05b0\u05e8\u05d5\u05bc\u0599', '168788')], [('\u05ea\u05b0\u05bc\u05e0\u05b4\u05a3\u05d9', '168789')], [('\u05d0\u05b6\u05ea', '168790'), ('\u05de\u05b7\u05db\u05b5\u05bc\u05a3\u05d4', '168791'), ('\u05d0\u05b8\u05d7\u05b4\u0597\u05d9\u05d5', '168792')], [('\u05d5\u05bc', '168793')], [('\u05e0\u05b0\u05de\u05b4\u05ea\u05b5\u05a8\u05d4\u05d5\u05bc\u0599', '168794')], [('\u05d1\u05b0\u05bc', '168795'), ('\u05e0\u05b6\u05a4\u05e4\u05b6\u05e9\u05c1', '168796'), ('\u05d0\u05b8\u05d7\u05b4\u05d9\u05d5\u0599', '168797')], [('\u05d0\u05b2\u05e9\u05b6\u05c1\u05a3\u05e8', '168798')], [('\u05d4\u05b8\u05e8\u05b8\u0594\u05d2', '168799')], [('\u05d5\u05b0', '168800')], [('\u05e0\u05b7\u05e9\u05b0\u05c1\u05de\u05b4\u0596\u05d9\u05d3\u05b8\u05d4', '168801')], [('\u05d2\u05b7\u05bc\u05a3\u05dd', '168802')], [('\u05d0\u05b6\u05ea', '168803'), ('\u05d4\u05b7', '168804'), ('\u05d9\u05b9\u05bc\u05d5\u05e8\u05b5\u0591\u05e9\u05c1', '168805')], [('\u05d5\u05b0', '168806')], [('\u05db\u05b4\u05d1\u05bc\u0597\u05d5\u05bc', '168807')], [('\u05d0\u05b6\u05ea', '168808'), ('\u05d2\u05b7\u05bc\u05bd\u05d7\u05b7\u05dc\u05b0\u05ea\u05b4\u05bc\u05d9\u0599', '168809')], [('\u05d0\u05b2\u05e9\u05b6\u05c1\u05a3\u05e8', '168810')], [('\u05e0\u05b4\u05e9\u05b0\u05c1\u05d0\u05b8\u0594\u05e8\u05b8\u05d4', '168811')], [('\u05dc\u05b0', '168812'), ('\u05d1\u05b4\u05dc\u05b0\u05ea\u05b4\u05bc\u05a7\u05d9', '168813')], [('\u05e9\u05b9\u05c2\u05d5\u05dd', '168814')], [('\u05dc\u05b0', '168815'), ('\u05d0\u05b4\u05d9\u05e9\u05b4\u05c1\u059b\u05d9', '168816')], [('\u05e9\u05b5\u05c1\u05a5\u05dd', '168817'), ('\u05d5\u05bc', '168818'), ('\u05e9\u05b0\u05c1\u05d0\u05b5\u05e8\u05b4\u0596\u05d9\u05ea', '168819')], [('\u05e2\u05b7\u05dc', '168820'), ('\u05e4\u05b0\u05bc\u05e0\u05b5\u05a5\u05d9', '168821'), ('\u05d4\u05b8', '168822'), ('\u05d0\u05b2\u05d3\u05b8\u05de\u05b8\u05bd\u05d4', '168823')]]\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Match rules with the database\n",
      "\n",
      "We compare the context part of each rule with the passage we find in the database.\n",
      "We look for all occurrences of the complete context in the elements of the specified passage. \n",
      "\n",
      "The ``match_elem`` function takes care of matching a context against verse material, and after that we run the matching for all rules.\n",
      "\n",
      "The outcome of a match is the list of numbers of the phrases in the verse where the match starts.\n",
      "Particularly interesting are the cases where there is not exactly one match."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def match_elem(context, elements):\n",
      "    matches = []\n",
      "    for (e, elem) in enumerate(elements):\n",
      "        equal = True\n",
      "        for (c, cont) in enumerate(context):\n",
      "            if len(elements) < e + c + 1 or cont[0] != elements[e + c][0]:\n",
      "                equal = False\n",
      "                break\n",
      "        if equal: matches.append(e)\n",
      "    return matches\n",
      "        \n",
      "hits = match_elem(context, verse['elems'])\n",
      "hits"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "[]"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "msg(\"Getting the passages material\")\n",
      "elements_dict = get_passage({r[0] for r in rules})\n",
      "msg(\"Done\")\n",
      "\n",
      "for (r, rule) in enumerate(rules):\n",
      "    (passage, target, context, offset) = rule\n",
      "    verse = elements_dict[passage]\n",
      "    verse_elems = verse['elems']\n",
      "    verse_text = verse['text']\n",
      "    matches = match_elem(context, verse_elems)\n",
      "    nm = len(matches)\n",
      "    print('{}: {} matches: {}'.format(passage, nm, matches))\n",
      "    if nm == 0:\n",
      "        print(\"NO MATCH in RULE {}: '{}' versus {}\\n{}\".format(\n",
      "            r, [t[0] for t in context], [\"{} - {}\".format(t[0][0], ','.join([w[1] for w in t[1]])) for t in zip(verse_elems, verse_text)], lines[r],\n",
      "        ))\n",
      "msg(\"Done\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 00s Getting the passages material\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 00s Look for passages\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 01s End walk\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 01s Done\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 01s Done\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('Exodus', '15', '25'): 1 matches: [13]\n",
        "('Exodus', '40', '8'): 1 matches: [0]\n",
        "('Leviticus', '6', '3'): 1 matches: [16]\n",
        "('Isaiah', '41', '19'): 1 matches: [3]\n",
        "('Isaiah', '42', '4'): 1 matches: [5]\n",
        "('Isaiah', '43', '19'): 1 matches: [8]\n",
        "('Isaiah', '57', '7'): 1 matches: [0]\n",
        "('Isaiah', '57', '8'): 1 matches: [0]\n",
        "('Ezekiel', '20', '28'): 1 matches: [20]\n",
        "('Habakkuk', '2', '9'): 1 matches: [4]\n",
        "('I_Samuel', '2', '20'): 1 matches: [6]\n",
        "('II_Samuel', '14', '7'): 0 matches: []\n",
        "NO MATCH in RULE 11: '['L BLTJ FWM', 'L >JCJ', 'CM W C>RJT', '<L PNJ H >DMH']' versus ['W - 168779', 'HNH - 168780', 'QMH - 168781', 'KL H MCPXH - 168782,168783,168784', '<L CPXTK - 168785,168786', 'W - 168787', 'J>MRW - 168788', 'TNJ - 168789', '>T MKH >XJW - 168790,168791,168792', 'W - 168793', 'NMTHW - 168794', 'B NPC >XJW - 168795,168796,168797', '>CR - 168798', 'HRG - 168799', 'W - 168800', 'NCMJDH - 168801', 'GM - 168802', '>T H JWRC - 168803,168804,168805', 'W - 168806', 'KBW - 168807', '>T GXLTJ - 168808,168809', '>CR - 168810', 'NC>RH - 168811', 'L BLTJ - 168812,168813', 'FWM - 168814', 'L >JCJ - 168815,168816', 'CM W C>RJT - 168817,168818,168819', '<L PNJ H >DMH - 168820,168821,168822,168823']\n",
        "IISA 14,07\t[L BLTJ FWM <Pr>]\t[L >JC +J <sc>]\t[CM W C>RJT <Ob>]\t[<L PNJ H >DMH <Co>]\t:CHANGESTO:\t[L >JC +J <Aj>]\n",
        "\n",
        "('I_Kings', '20', '34'): 1 matches: [9]\n",
        "('II_Kings', '4', '10'): 1 matches: [3]\n",
        "('Isaiah', '21', '4'): 1 matches: [4]\n",
        "('I_Samuel', '19', '13'): 1 matches: [7]\n",
        "('II_Kings', '10', '8'): 1 matches: [11]\n",
        "('II_Kings', '13', '7'): 1 matches: [10]\n",
        "('Isaiah', '53', '10'): 1 matches: [5]\n",
        "('Jeremiah', '11', '13'): 1 matches: [5]\n",
        "('Ezekiel', '17', '5'): 1 matches: [8]\n",
        "('Ezekiel', '19', '5'): 1 matches: [10]\n",
        "('Hosea', '2', '5'): 1 matches: [7]\n",
        "('Micah', '1', '7'): 1 matches: [7]\n",
        "('Micah', '2', '12'): 0 matches: []\n",
        "NO MATCH in RULE 24: '['JXD', '>FJMNW', 'K Y>N BYRH']' versus ['>SP - 301667', '>>SP - 301668', 'J<QB - 301669', 'KLK - 301670', 'QBY - 301671', '>QBY - 301672', 'C>RJT JFR>L - 301673,301674', 'JXD - 301675', '>FJMNW - 301676', 'K Y>N - 301677,301678', 'BYRH - 301679', 'K <DR - 301680,301681', 'B TWK H DBRW - 301682,301683,301684,301685', 'THJMNH - 301686', 'M >DM - 301687,301688']\n",
        "MICH02,12\t[JXD <Mo>]\t[>FJMN +W <PO>]\t[K Y>N BYRH <Aj>]\t:CHANGESTO:\t[K Y>N BYRH <Co>]\n",
        "\n",
        "('Psalms', '89', '30'): 1 matches: [0]\n",
        "('Job', '38', '9'): 1 matches: [0]\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Implementing Flow Charts\n",
      "\n",
      "1. Pick one or two flow charts by Janet.\n",
      "2. translate them in a model.\n",
      "3. Use the model to analyse relevant occurrences of phrases with verbs\n",
      "4. Produce the output of the analysis.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}