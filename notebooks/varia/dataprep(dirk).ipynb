{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Towards a valency dictionary"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A valency dictionary is a kind of flow chart by which you can separate out verb meanings based on the number and nature of the complements they take. We want to be able to automatically match contexts against flow charts.\n",
      "\n",
      "The work in this notebook consists of two parts: applying corrections to the data and implementing flow charts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import collections\n",
      "\n",
      "from laf.fabric import LafFabric\n",
      "fabric = LafFabric()\n",
      "from etcbc.preprocess import prepare"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s This is LAF-Fabric 4.0.3\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fabric.load('bhs3.txt.hdr', '--', 'dataprep',\n",
      "        {\n",
      "            \"xmlids\": {\n",
      "                \"node\": False,\n",
      "                \"edge\": False,\n",
      "            },\n",
      "            \"features\": {\n",
      "                \"shebanq\": {\n",
      "                    \"node\": [\n",
      "                        \"db.otype,monads\",\n",
      "                        \"ft.text,surface_consonants\",\n",
      "                        \"ft.phrase_function,locative\",\n",
      "                        \"sft.book,chapter,verse_label,verse\",\n",
      "                    ],\n",
      "                    \"edge\": [\n",
      "                    ],\n",
      "                },\n",
      "            },\n",
      "            'prepare': prepare,\n",
      "        },\n",
      "        compile_main=False, compile_annox=False,\n",
      "        verbose='DETAIL',\n",
      "    )\n",
      "\n",
      "exec(fabric.localnames.format(var='fabric'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s LOADING API: please wait ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING m: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING a: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: load main: G.node_anchor_min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.11s DETAIL: load main: G.node_anchor_max\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.20s DETAIL: load main: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.29s DETAIL: load main: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.75s DETAIL: load main: G.edges_from\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.84s DETAIL: load main: G.edges_to\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.92s DETAIL: load main: F.shebanq_db_otype [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.58s DETAIL: load main: F.shebanq_db_monads [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.39s DETAIL: load main: F.shebanq_ft_text [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.72s DETAIL: load main: F.shebanq_ft_surface_consonants [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.92s DETAIL: load main: F.shebanq_ft_phrase_function [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.04s DETAIL: load main: F.shebanq_ft_locative [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.23s DETAIL: load main: F.shebanq_sft_book [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.25s DETAIL: load main: F.shebanq_sft_chapter [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.26s DETAIL: load main: F.shebanq_sft_verse_label [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.28s DETAIL: load main: F.shebanq_sft_verse [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.29s LOGFILE=/Users/judith/laf-fabric-data/etcbc-bhs3/tasks/bhs3.txt.hdr/dataprep/__log__dataprep.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.30s DETAIL: prep prep: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.40s DETAIL: prep prep: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.90s INFO: DATA LOADED FROM SOURCE bhs3.txt.hdr AND ANNOX -- FOR TASK dataprep\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Applying corrections on the parsing analysis of the data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Janet has files with nearly formal corrections to the parsing analysis that has been coded into the BHS3 version of the Hebrew Text Database.\n",
      "\n",
      "We are going to translate those instructions into new LAF annotations: an annox package.\n",
      "\n",
      "# Specification of steps ahead\n",
      "\n",
      "1. In the corrections file, find out which of the phrases before :CHANGESTO: has to receive a modified phrase function.\n",
      "   This is the target phrase.\n",
      "2. Match the target phrase with a phrase occurrence in the indicated passage in the database.\n",
      "3. Produce an annoting input file, tab separated, 2 columns: the oid of the target phrase, the modified phrase_function.\n",
      "   The name of this column should be shebanq:ft.phrase_function\n",
      "4. Use the module etcbc.annotating (and the method make_form to produce a file of annotations\n",
      "5. put the file of annotations in a new annox (in the annotations directory)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Modeling the correction instructions\n",
      "\n",
      "We model the information in each rule in a datastructure, containing all the bits needed for the rest of the tasks in a readily available form.\n",
      "\n",
      "Have a look at the file with corrections, we show just one line here.\n",
      "We apply some consistency in book names and parsing labels between the ways they are represented in the corrections file and in the database."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "handle = open('CorrectionsFJM.txt', encoding=\"utf8\")\n",
      "lines = [line for line in handle]\n",
      "handle.close()\n",
      "lines[3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "'JES 41,19\\t[>FJM <Pr>]\\t[B <RBH <Lo>]\\t[BRWC TDHR W T>CWR <Ob>]\\t[JXDW <Mo>]\\t:CHANGESTO:\\t[B <RBH <Co>]\\n'"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "code_def = {\n",
      " '..': '..',\n",
      " 'Aj': 'Adju',\n",
      " 'Cj': 'Conj',\n",
      " 'Co': 'Cmpl',\n",
      " 'Lo': 'Loca',\n",
      " 'Mo': 'Modi',\n",
      " 'Ob': 'Objc',\n",
      " 'PO': 'PreO',\n",
      " 'Pr': 'Pred',\n",
      " 'Ps': 'PreS',\n",
      " 'Su': 'Subj',\n",
      " 'Ti': 'Time',\n",
      " 'sc': 'Supp',\n",
      "}\n",
      "\n",
      "book_def = {\n",
      "    'GEN': 'Genesis',\n",
      "    'EXO': 'Exodus',\n",
      "    'LEV': 'Leviticus',\n",
      "    'NUM': 'Numbers',\n",
      "    'DEU': 'Deuteronomy',\n",
      "    'JOS': 'Joshua',\n",
      "    'JUD': 'Judges',\n",
      "    'ISAM': 'I_Samuel',\n",
      "    'IISA': 'II_Samuel',\n",
      "    'IKON': 'I_Kings',\n",
      "    'IIKON': 'II_Kings',\n",
      "    'JES': 'Isaiah',\n",
      "    'JER': 'Jeremiah',\n",
      "    'EZE': 'Ezekiel',\n",
      "    'HOS': 'Hosea',\n",
      "    'JOE': 'Joel',\n",
      "    'AMO': 'Amos',\n",
      "    'OBA': 'Obadiah',\n",
      "    'JON': 'Jonah',\n",
      "    'MICH': 'Micah',\n",
      "    'NAH': 'Nahum',\n",
      "    'HAB': 'Habakkuk',\n",
      "    'ZEP': 'Zephaniah',\n",
      "    'HAG': 'Haggai',\n",
      "    'ZEC': 'Zechariah',\n",
      "    'MAL': 'Malachi',\n",
      "    'PS': 'Psalms',\n",
      "    'IOB': 'Job',\n",
      "    'PRO': 'Proverbs',\n",
      "    'RUT': 'Ruth',\n",
      "    'CAN': 'Canticles',\n",
      "    'ECC': 'Ecclesiastes',\n",
      "    'LAM': 'Lamentations',\n",
      "    'EST': 'Esther',\n",
      "    'DAN': 'Daniel',\n",
      "    'EZR': 'Ezra',\n",
      "    'NEH': 'Nehemiah',\n",
      "    'I_c': 'I_Chronicles',\n",
      "    'Ii_': 'II_Chronicles',    \n",
      "}\n",
      "\n",
      "def compile_rules(lines):\n",
      "    rules = []\n",
      "    for line in lines:\n",
      "        line = line.strip()\n",
      "        raw_all_fields = line.split('\\t')\n",
      "        passage = raw_all_fields[0]\n",
      "        p_comp = re.findall('^([A-Z]+)\\s*0*([0-9]+)\\s*,\\s*0*([0-9]+)\\s*$', passage)[0]\n",
      "        raw_fields = raw_all_fields[1:-2] + [raw_all_fields[-1]]\n",
      "        string_fields = [f.strip('[]') for f in raw_fields]\n",
      "        fields = [f.rsplit('<', 1) for f in string_fields]\n",
      "        new_fields = [(re.sub('\\s*\\+', '', f[0].rstrip()), code_def[f[1].rstrip('>')]) for f in fields]\n",
      "        rules.append([(book_def[p_comp[0]], p_comp[1], p_comp[2]), new_fields[-1], tuple(new_fields[0:-1])])\n",
      "    \n",
      "    for (i, rule) in enumerate(rules):\n",
      "        (passage, target, context) = rule\n",
      "        (target_words, target_code) = target\n",
      "        hits = []\n",
      "        the_hit = -1\n",
      "        for (n, cw) in enumerate(context):\n",
      "            if cw[0] == target_words: hits.append(n)\n",
      "        if not hits:\n",
      "            msg(\"WARNING: Rule {} [{}]: Nothing fits {} in context {}\".format(i+1, passage, target_words, context))\n",
      "        elif len(hits) > 1:\n",
      "            msg(\"WARNING: Rule {} [{}]: More than one phrase fit {} in context {}\".format(i+1, passage, target_words, context))\n",
      "            the_hit = hits[-1]\n",
      "        else: the_hit = hits[-1]\n",
      "        rule.append(the_hit)\n",
      "    return rules"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Retrieving passages\n",
      "\n",
      "We examine all the rules, collect the passages that we have to address, and collect data from them from the database."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_passage(passages):\n",
      "    cur_book = None\n",
      "    cur_chapter = None\n",
      "    cur_passage = None\n",
      "    cur_phrase = [None, []]\n",
      "    verse_phrases = []\n",
      "    \n",
      "    passages_dict = {}\n",
      "    inpassage = False\n",
      "    msg('Look for passages')\n",
      "    for n in NN():\n",
      "        otype = F.otype.v(n)\n",
      "        if otype == 'book':\n",
      "            cur_book = F.book.v(n)\n",
      "        elif otype == 'chapter':\n",
      "            cur_chapter = F.chapter.v(n)\n",
      "        elif otype == 'verse':\n",
      "            cur_verse = F.verse.v(n)\n",
      "            if cur_phrase[0] != None: verse_phrases.append(cur_phrase)\n",
      "            cur_phrase = [None, []]\n",
      "            if verse_phrases:\n",
      "                passages_dict[cur_passage] = {\n",
      "                    'elems': [\n",
      "                        (' '.join([F.surface_consonants.v(y) for y in x[1]]), F.phrase_function.v(x[0])) for x in verse_phrases\n",
      "                    ],\n",
      "                    'text': [\n",
      "                        [(F.text.v(y), F.monads.v(y)) for y in x[1]] for x in verse_phrases\n",
      "                    ],\n",
      "                }\n",
      "            verse_phrases = []\n",
      "            if (cur_book, cur_chapter, cur_verse) in passages: \n",
      "                cur_passage = (cur_book, cur_chapter, cur_verse)\n",
      "                inpassage = True\n",
      "                cur_phrase = [None, []]\n",
      "                verse_phrases = []\n",
      "            else: inpassage = False\n",
      "        elif inpassage:\n",
      "            if otype == 'word': \n",
      "                if F.surface_consonants.v(n) != '.': cur_phrase[1].append(n)   \n",
      "            elif otype == 'phrase':\n",
      "                if cur_phrase[0] != None: verse_phrases.append(cur_phrase)\n",
      "                cur_phrase = [n, []]\n",
      "    if cur_phrase[0] != None: verse_phrases.append(cur_phrase)\n",
      "    if verse_phrases:\n",
      "        passages_dict[cur_passage] = {\n",
      "            'elems': [\n",
      "                (' '.join([F.surface_consonants.v(y) for y in x[1]]), F.phrase_function.v(x[0])) for x in verse_phrases\n",
      "            ],\n",
      "            'text': [\n",
      "                [(F.text.v(y), F.monads.v(y)) for y in x[1]] for x in verse_phrases\n",
      "            ],\n",
      "        }\n",
      "    msg(\"End walk\")\n",
      "    return passages_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "rules = compile_rules(lines)\n",
      "show = 11\n",
      "rule = rules[show]\n",
      "(passage, target, context, offset) = rule\n",
      "verse = get_passage({passage})[passage]\n",
      "print(\"RULE {}=\\n{}\\nVERSE ELEMS:{}\\nVERSE TEXT={}\".format(show, rule, verse['elems'], verse['text']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1h 22m 20s Look for passages\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1h 22m 21s End walk\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RULE 11=\n",
        "[('II_Samuel', '14', '7'), ('L >JCJ', 'Adju'), (('L BLTJ FWM', 'Pred'), ('L >JCJ', 'Supp'), ('CM W C>RJT', 'Objc'), ('<L PNJ H >DMH', 'Cmpl')), 1]\n",
        "VERSE ELEMS:[('W', 'Conj'), ('HNH', 'Intj'), ('QMH', 'Pred'), ('KL H MCPXH', 'Subj'), ('<L CPXTK', 'Cmpl'), ('W', 'Conj'), ('J>MRW', 'Pred'), ('TNJ', 'Pred'), ('>T MKH >XJW', 'Objc'), ('W', 'Conj'), ('NMTHW', 'PreO'), ('B NPC >XJW', 'Adju'), ('>CR', 'Rela'), ('HRG', 'Pred'), ('W', 'Conj'), ('NCMJDH', 'Pred'), ('GM', 'Modi'), ('>T H JWRC', 'Objc'), ('W', 'Conj'), ('KBW', 'Pred'), ('>T GXLTJ', 'Objc'), ('>CR', 'Rela'), ('NC>RH', 'Pred'), ('L BLTJ', 'Conj'), ('FWM', 'Pred'), ('L >JCJ', 'Supp'), ('CM W C>RJT', 'Objc'), ('<L PNJ H >DMH', 'Cmpl')]\n",
        "VERSE TEXT=[[('\u05d5\u05b0', '168779')], [('\u05d4\u05b4\u05e0\u05b5\u05bc\u05d4\u05a9', '168780')], [('\u05e7\u05b8\u05a8\u05de\u05b8\u05d4', '168781')], [('\u05db\u05b8\u05bd\u05dc', '168782'), ('\u05d4\u05b7', '168783'), ('\u05de\u05b4\u05bc\u05e9\u05b0\u05c1\u05e4\u05b8\u05bc\u05d7\u05b8\u059c\u05d4', '168784')], [('\u05e2\u05b7\u05dc', '168785'), ('\u05e9\u05b4\u05c1\u05e4\u05b0\u05d7\u05b8\u05ea\u05b6\u0597\u05da\u05b8', '168786')], [('\u05d5\u05b7', '168787')], [('\u05d9\u05b9\u05bc\u05bd\u05d0\u05de\u05b0\u05e8\u05d5\u05bc\u0599', '168788')], [('\u05ea\u05b0\u05bc\u05e0\u05b4\u05a3\u05d9', '168789')], [('\u05d0\u05b6\u05ea', '168790'), ('\u05de\u05b7\u05db\u05b5\u05bc\u05a3\u05d4', '168791'), ('\u05d0\u05b8\u05d7\u05b4\u0597\u05d9\u05d5', '168792')], [('\u05d5\u05bc', '168793')], [('\u05e0\u05b0\u05de\u05b4\u05ea\u05b5\u05a8\u05d4\u05d5\u05bc\u0599', '168794')], [('\u05d1\u05b0\u05bc', '168795'), ('\u05e0\u05b6\u05a4\u05e4\u05b6\u05e9\u05c1', '168796'), ('\u05d0\u05b8\u05d7\u05b4\u05d9\u05d5\u0599', '168797')], [('\u05d0\u05b2\u05e9\u05b6\u05c1\u05a3\u05e8', '168798')], [('\u05d4\u05b8\u05e8\u05b8\u0594\u05d2', '168799')], [('\u05d5\u05b0', '168800')], [('\u05e0\u05b7\u05e9\u05b0\u05c1\u05de\u05b4\u0596\u05d9\u05d3\u05b8\u05d4', '168801')], [('\u05d2\u05b7\u05bc\u05a3\u05dd', '168802')], [('\u05d0\u05b6\u05ea', '168803'), ('\u05d4\u05b7', '168804'), ('\u05d9\u05b9\u05bc\u05d5\u05e8\u05b5\u0591\u05e9\u05c1', '168805')], [('\u05d5\u05b0', '168806')], [('\u05db\u05b4\u05d1\u05bc\u0597\u05d5\u05bc', '168807')], [('\u05d0\u05b6\u05ea', '168808'), ('\u05d2\u05b7\u05bc\u05bd\u05d7\u05b7\u05dc\u05b0\u05ea\u05b4\u05bc\u05d9\u0599', '168809')], [('\u05d0\u05b2\u05e9\u05b6\u05c1\u05a3\u05e8', '168810')], [('\u05e0\u05b4\u05e9\u05b0\u05c1\u05d0\u05b8\u0594\u05e8\u05b8\u05d4', '168811')], [('\u05dc\u05b0', '168812'), ('\u05d1\u05b4\u05dc\u05b0\u05ea\u05b4\u05bc\u05a7\u05d9', '168813')], [('\u05e9\u05b9\u05c2\u05d5\u05dd', '168814')], [('\u05dc\u05b0', '168815'), ('\u05d0\u05b4\u05d9\u05e9\u05b4\u05c1\u059b\u05d9', '168816')], [('\u05e9\u05b5\u05c1\u05a5\u05dd', '168817'), ('\u05d5\u05bc', '168818'), ('\u05e9\u05b0\u05c1\u05d0\u05b5\u05e8\u05b4\u0596\u05d9\u05ea', '168819')], [('\u05e2\u05b7\u05dc', '168820'), ('\u05e4\u05b0\u05bc\u05e0\u05b5\u05a5\u05d9', '168821'), ('\u05d4\u05b8', '168822'), ('\u05d0\u05b2\u05d3\u05b8\u05de\u05b8\u05bd\u05d4', '168823')]]\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Match rules with the database\n",
      "\n",
      "We compare the context part of each rule with the passage we find in the database.\n",
      "We look for all occurrences of the complete context in the elements of the specified passage. \n",
      "\n",
      "The ``match_elem`` function takes care of matching a context against verse material, and after that we run the matching for all rules.\n",
      "\n",
      "The outcome of a match is the list of numbers of the phrases in the verse where the match starts.\n",
      "Particularly interesting are the cases where there is not exactly one match."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def match_elem(context, elements):\n",
      "    matches = []\n",
      "    for (e, elem) in enumerate(elements):\n",
      "        equal = True\n",
      "        for (c, cont) in enumerate(context):\n",
      "            if len(elements) < e + c + 1 or cont[0] != elements[e + c][0]:\n",
      "                equal = False\n",
      "                break\n",
      "        if equal: matches.append(e)\n",
      "    return matches\n",
      "        \n",
      "hits = match_elem(context, verse['elems'])\n",
      "hits"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "[8]"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "msg(\"Getting the passages material\")\n",
      "elements_dict = get_passage({r[0] for r in rules})\n",
      "msg(\"Done\")\n",
      "\n",
      "for (r, rule) in enumerate(rules):\n",
      "    (passage, target, context, offset) = rule\n",
      "    verse = elements_dict[passage]\n",
      "    verse_elems = verse['elems']\n",
      "    verse_text = verse['text']\n",
      "    matches = match_elem(context, verse_elems)\n",
      "    nm = len(matches)\n",
      "    print('{}: {} matches: {}'.format(passage, nm, matches))\n",
      "    if nm == 0:\n",
      "        print(\"NO MATCH in RULE {}: '{}' versus {}\\n{}\".format(\n",
      "            r, [t[0] for t in context], [\"{} - {}\".format(t[0][0], ','.join([w[1] for w in t[1]])) for t in zip(verse_elems, verse_text)], lines[r],\n",
      "        ))\n",
      "msg(\"Done\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "48m 04s Getting the passages material\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "48m 04s Look for passages\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "48m 06s End walk\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "48m 06s Done\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "48m 06s Done\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('Exodus', '15', '25'): 1 matches: [13]\n",
        "('Exodus', '40', '8'): 1 matches: [0]\n",
        "('Leviticus', '6', '3'): 1 matches: [16]\n",
        "('Isaiah', '41', '19'): 1 matches: [3]\n",
        "('Isaiah', '42', '4'): 1 matches: [5]\n",
        "('Isaiah', '43', '19'): 1 matches: [8]\n",
        "('Isaiah', '57', '7'): 1 matches: [0]\n",
        "('Isaiah', '57', '8'): 1 matches: [0]\n",
        "('Ezekiel', '20', '28'): 1 matches: [20]\n",
        "('Habakkuk', '2', '9'): 1 matches: [4]\n",
        "('I_Samuel', '2', '20'): 1 matches: [6]\n",
        "('II_Samuel', '14', '7'): 0 matches: []\n",
        "NO MATCH in RULE 11: '['L BLTJ FWM', 'L >JCJ', 'CM W C>RJT', '<L PNJ H >DMH']' versus ['W - 168779', 'HNH - 168780', 'QMH - 168781', 'KL H MCPXH - 168782,168783,168784', '<L CPXTK - 168785,168786', 'W - 168787', 'J>MRW - 168788', 'TNJ - 168789', '>T MKH >XJW - 168790,168791,168792', 'W - 168793', 'NMTHW - 168794', 'B NPC >XJW - 168795,168796,168797', '>CR - 168798', 'HRG - 168799', 'W - 168800', 'NCMJDH - 168801', 'GM - 168802', '>T H JWRC - 168803,168804,168805', 'W - 168806', 'KBW - 168807', '>T GXLTJ - 168808,168809', '>CR - 168810', 'NC>RH - 168811', 'L BLTJ - 168812,168813', 'FWM - 168814', 'L >JCJ - 168815,168816', 'CM W C>RJT - 168817,168818,168819', '<L PNJ H >DMH - 168820,168821,168822,168823']\n",
        "IISA 14,07\t[L BLTJ FWM <Pr>]\t[L >JC +J <sc>]\t[CM W C>RJT <Ob>]\t[<L PNJ H >DMH <Co>]\t:CHANGESTO:\t[L >JC +J <Aj>]\n",
        "\n",
        "('I_Kings', '20', '34'): 1 matches: [9]\n",
        "('II_Kings', '4', '10'): 1 matches: [3]\n",
        "('Isaiah', '21', '4'): 1 matches: [4]\n",
        "('I_Samuel', '19', '13'): 1 matches: [7]\n",
        "('II_Kings', '10', '8'): 1 matches: [11]\n",
        "('II_Kings', '13', '7'): 1 matches: [10]\n",
        "('Isaiah', '53', '10'): 1 matches: [5]\n",
        "('Jeremiah', '11', '13'): 1 matches: [5]\n",
        "('Ezekiel', '17', '5'): 1 matches: [8]\n",
        "('Ezekiel', '19', '5'): 1 matches: [10]\n",
        "('Hosea', '2', '5'): 1 matches: [7]\n",
        "('Micah', '1', '7'): 1 matches: [7]\n",
        "('Micah', '2', '12'): 0 matches: []\n",
        "NO MATCH in RULE 24: '['JXD', '>FJMNW', 'K Y>N BYRH']' versus ['>SP - 301667', '>>SP - 301668', 'J<QB - 301669', 'KLK - 301670', 'QBY - 301671', '>QBY - 301672', 'C>RJT JFR>L - 301673,301674', 'JXD - 301675', '>FJMNW - 301676', 'K Y>N - 301677,301678', 'BYRH - 301679', 'K <DR - 301680,301681', 'B TWK H DBRW - 301682,301683,301684,301685', 'THJMNH - 301686', 'M >DM - 301687,301688']\n",
        "MICH02,12\t[JXD <Mo>]\t[>FJMN +W <PO>]\t[K Y>N BYRH <Aj>]\t:CHANGESTO:\t[K Y>N BYRH <Co>]\n",
        "\n",
        "('Psalms', '89', '30'): 1 matches: [0]\n",
        "('Job', '38', '9'): 1 matches: [0]\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Implementing Flow Charts\n",
      "\n",
      "1. Pick one or two flow charts by Janet.\n",
      "2. translate them in a model.\n",
      "3. Use the model to analyse relevant occurrences of phrases with verbs\n",
      "4. Produce the output of the analysis.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}