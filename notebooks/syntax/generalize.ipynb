{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import collections\n",
      "\n",
      "from laf.fabric import LafFabric\n",
      "processor = LafFabric(verbose='DETAIL')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s This is LAF-Fabric 4.0.2\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "API = processor.load('bhs3.txt.hdr', '--', 'generalization', {\n",
      "    \"primary\": False,\n",
      "    \"xmlids\": {\n",
      "        \"node\": False,\n",
      "        \"edge\": False,\n",
      "    },\n",
      "    \"features\": {\n",
      "        \"shebanq\": {\n",
      "            \"node\": [\n",
      "                \"db.otype\",\n",
      "                \"ft.phrase_function\",\n",
      "                \"sft.book,chapter,verse,verse_label\",\n",
      "            ],\n",
      "            \"edge\": [\n",
      "            ],\n",
      "        },\n",
      "    },\n",
      "})\n",
      "\n",
      "F = API['F']\n",
      "NN = API['NN']\n",
      "infile = API['infile']\n",
      "outfile = API['outfile']\n",
      "my_file = API['my_file']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s LOADING API: please wait ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING m: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING a: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: load main: G.node_anchor_min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.12s DETAIL: load main: G.node_anchor_max\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.21s DETAIL: load main: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.30s DETAIL: load main: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.75s DETAIL: load main: G.edges_from\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.83s DETAIL: load main: G.edges_to\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.92s DETAIL: load main: F.shebanq_db_otype [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.58s DETAIL: load main: F.shebanq_ft_phrase_function [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.71s DETAIL: load main: F.shebanq_sft_book [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.72s DETAIL: load main: F.shebanq_sft_chapter [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.74s DETAIL: load main: F.shebanq_sft_verse [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.75s DETAIL: load main: F.shebanq_sft_verse_label [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.77s LOGFILE=/Users/judith/laf-fabric-data/etcbc-bhs3/tasks/bhs3.txt.hdr/generalization/__log__generalization.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.77s INFO: DATA LOADED FROM SOURCE bhs3.txt.hdr AND ANNOX -- FOR TASK generalization\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ofile = outfile(\"output.txt\")\n",
      "\n",
      "def generalize(section, ofile):\n",
      "    section = collections.defaultdict(list)\n",
      "    count = 0\n",
      "    frequency = 0\n",
      "    \n",
      "    for node in NN(test=F.shebanq_db_otype.v, values=['book', 'chapter', 'verse', 'phrase']):\n",
      "        if F.shebanq_db_otype.v(node) == 'book':\n",
      "            the_book = F.shebanq_db_otype.v(node)\n",
      "        elif F.shebanq_db_otype.v(node) == \"chapter\":\n",
      "            the_chapter = F.shebanq_sft_chapter.v(node)\n",
      "        elif F.shebanq_db_otype.v(node) == 'verse' and section_type == 'verse':\n",
      "            the_verse = F.shebanq_sft_verse.v(node)\n",
      "        else:\n",
      "            if F.shebanq_db_otype.v(node) == 'phrase':\n",
      "                if section_type == 'verse':\n",
      "                    passage = the_book + ' ' + the_chapter + ' ' + the_verse\n",
      "                    section[passage].append(node)\n",
      "                elif section_type == 'chapter':\n",
      "                    passage = the_book + ' ' + the_chapter\n",
      "                    section[passage].append(node)  \n",
      " \n",
      "    check = 0\n",
      "    for keys, values in section.items():\n",
      "            length = len(values)\n",
      "            check = check + 1\n",
      "            for element in values:\n",
      "                if F.shebanq_ft_phrase_function.v(element) == 'Subj':\n",
      "                    count = count + 1\n",
      "                    frequency = count / length\n",
      "            ofile.write(\"{} Phrases: {} Subjects: {} Frequency {}\\n\".format(keys, length, count, frequency)) \n",
      "            count = 0  \n",
      "    \n",
      "    \n",
      "section_type = 'verse'\n",
      "    \n",
      "generalize(section_type, ofile)\n",
      "\n",
      "\n",
      "                  \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}