{
 "metadata": {
  "name": "",
  "signature": "sha256:61bc3b1b38d4b2eecdfdc4627c5a8d33edede4f564dda37fe3a5d351871e0088"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Towards a persuasive valence dictionary"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goals of this notebook for the ETCBC is to correct parsing mistakes in the ETCBC database; as an extension of this tool we will also list single representatives of diverse valence patterns in a digital dictionary format and extent it to a full-fledged valence dictionary\n",
      "\n",
      "Such a valency dictionary is a kind of flow chart by which it is possible to determine verb meanings based on the number and nature of the complements they take. With the valence dictionary we want to be able to automatically match contexts against flow charts. The work in this notebook consists of two parts: applying corrections to the data and implementing flow charts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import collections\n",
      "\n",
      "from laf.fabric import LafFabric\n",
      "fabric = LafFabric()\n",
      "from etcbc.preprocess import prepare"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s This is LAF-Fabric 4.2.8\n",
        "http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "API = fabric.load('bhs3', '--', 'dataprep',\n",
      "        {\n",
      "            \"xmlids\": {\n",
      "                \"node\": True,\n",
      "                \"edge\": True,\n",
      "            },\n",
      "            \"features\": {\n",
      "                \"shebanq\": {\n",
      "                    \"node\": [\n",
      "                        \"db.otype,monads\",\n",
      "                        \"ft.text,surface_consonants\",\n",
      "                        \"ft.phrase_function,locative\",\n",
      "                        \"sft.book,chapter,verse_label,verse\",\n",
      "                    ],\n",
      "                    \"edge\": [\n",
      "                    ],\n",
      "                },\n",
      "            },\n",
      "            'prepare': prepare,\n",
      "        },\n",
      "        compile_main=False, compile_annox=False,\n",
      "        verbose='DETAIL',\n",
      "    )\n",
      "\n",
      "exec(fabric.localnames.format(var='fabric'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s LOADING API: please wait ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING m: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s INFO: DATA COMPILED AT: 2014-04-18T18-24-37\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING a: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: load main: G.node_anchor_min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.11s DETAIL: load main: G.node_anchor_max\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.20s DETAIL: load main: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.30s DETAIL: load main: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.82s DETAIL: load main: G.edges_from\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.91s DETAIL: load main: G.edges_to\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.00s DETAIL: load main: X. [node]  -> \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.37s DETAIL: load main: X. [e]  -> \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.81s DETAIL: load main: X. [node]  <- \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.60s DETAIL: load main: X. [e]  <- \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.46s DETAIL: load main: F.shebanq_db_monads [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  6.38s DETAIL: load main: F.shebanq_db_otype [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  7.23s DETAIL: load main: F.shebanq_ft_locative [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  7.54s DETAIL: load main: F.shebanq_ft_phrase_function [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  7.74s DETAIL: load main: F.shebanq_ft_surface_consonants [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  7.99s DETAIL: load main: F.shebanq_ft_text [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  8.37s DETAIL: load main: F.shebanq_sft_book [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  8.39s DETAIL: load main: F.shebanq_sft_chapter [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  8.41s DETAIL: load main: F.shebanq_sft_verse [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  8.42s DETAIL: load main: F.shebanq_sft_verse_label [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  8.44s LOGFILE=/Users/judith/laf-fabric-data/bhs3/tasks/dataprep/__log__dataprep.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  8.44s DETAIL: prep prep: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  8.55s DETAIL: prep prep: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.14s INFO: DATA LOADED FROM SOURCE bhs3 AND ANNOX -- FOR TASK dataprep\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Applying corrections to the ETCBC database"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Based on the val2csv-script developed by Ulrik Sandborg-Petersen we have received nearly formal corrections to the parsing analysis which have been coded into the 3BHS version of the Hebrew text database. The task of this notebook is to translate these correction into new LAF annotations and to this way create a new annotation file as addition to the LAF version of the ETCBC database"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the following notebook we are applying the following steps to create an annotation file:"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Specification of the steps ahead:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. In the corrections file, find out which of the phrases before :CHANGESTO: has to receive a modified phrase function. This is the target phrase.\n",
      "2. Match the target phrase with a phrase occurrence in the indicated passage in the database.\n",
      "3. Produce an annoting input file, tab separated, 2 columns: the oid of the target phrase, the modified phrase_function. The name of this column should be shebanq:ft.phrase_function\n",
      "4. Use the module etcbc.annotating (and the method make_form to produce a file of annotations\n",
      "5. put the file of annotations in a new annox (in the annotations directory)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Modelling the correction instructions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The corrections file contains the linguistic context within a verse plus the phrase which needs to be corrected. We model the information from these rules in the corrections file in a data structure. This data structure contains all the bits needed for the rest of the correction phrases: We generate the rules as a list which contain the exact location in the Bible as book, chapter and verses on position 0 in the list, the target phrase which contains the corrections on position 1 in the list and then the context in which the change occurs in the rest of the list. Position -1 contains an offset indicating the phrase in the list which is to be changed. \n",
      "\n",
      "Have a look at the file with corrections, we show just one line here. We apply some consistency in book names and parsing labels between the ways they are represented in the corrections file and in the database."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "handle = open('CorrectionsFJM.txt', encoding=\"utf8\")\n",
      "lines = [line for line in handle]\n",
      "handle.close()\n",
      "lines[3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "'JES 41,19\\t[>FJM <Pr>]\\t[B <RBH <Lo>]\\t[BRWC TDHR W T>CWR <Ob>]\\t[JXDW <Mo>]\\t:CHANGESTO:\\t[B <RBH <Co>]\\n'"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "code_def = {\n",
      " '..': '..',\n",
      " 'Aj': 'Adju',\n",
      " 'Cj': 'Conj',\n",
      " 'Co': 'Cmpl',\n",
      " 'Lo': 'Loca',\n",
      " 'Mo': 'Modi',\n",
      " 'Ob': 'Objc',\n",
      " 'PO': 'PreO',\n",
      " 'Pr': 'Pred',\n",
      " 'Ps': 'PreS',\n",
      " 'Su': 'Subj',\n",
      " 'Ti': 'Time',\n",
      " 'sc': 'Supp',\n",
      "}\n",
      "\n",
      "book_def = {\n",
      "    'GEN': 'Genesis',\n",
      "    'EXO': 'Exodus',\n",
      "    'LEV': 'Leviticus',\n",
      "    'NUM': 'Numbers',\n",
      "    'DEU': 'Deuteronomy',\n",
      "    'JOS': 'Joshua',\n",
      "    'JUD': 'Judges',\n",
      "    'ISAM': 'I_Samuel',\n",
      "    'IISA': 'II_Samuel',\n",
      "    'IKON': 'I_Kings',\n",
      "    'IIKON': 'II_Kings',\n",
      "    'JES': 'Isaiah',\n",
      "    'JER': 'Jeremiah',\n",
      "    'EZE': 'Ezekiel',\n",
      "    'HOS': 'Hosea',\n",
      "    'JOE': 'Joel',\n",
      "    'AMO': 'Amos',\n",
      "    'OBA': 'Obadiah',\n",
      "    'JON': 'Jonah',\n",
      "    'MICH': 'Micah',\n",
      "    'NAH': 'Nahum',\n",
      "    'HAB': 'Habakkuk',\n",
      "    'ZEP': 'Zephaniah',\n",
      "    'HAG': 'Haggai',\n",
      "    'ZEC': 'Zechariah',\n",
      "    'MAL': 'Malachi',\n",
      "    'PS': 'Psalms',\n",
      "    'IOB': 'Job',\n",
      "    'PRO': 'Proverbs',\n",
      "    'RUT': 'Ruth',\n",
      "    'CAN': 'Canticles',\n",
      "    'ECC': 'Ecclesiastes',\n",
      "    'LAM': 'Lamentations',\n",
      "    'EST': 'Esther',\n",
      "    'DAN': 'Daniel',\n",
      "    'EZR': 'Ezra',\n",
      "    'NEH': 'Nehemiah',\n",
      "    'I_c': 'I_Chronicles',\n",
      "    'Ii_': 'II_Chronicles',    \n",
      "}\n",
      "\n",
      "def compile_rules(lines):\n",
      "    rules = []\n",
      "    for line in lines:\n",
      "        line = line.strip()\n",
      "        raw_all_fields = line.split('\\t')\n",
      "        passage = raw_all_fields[0]\n",
      "        p_comp = re.findall('^([A-Z]+)\\s*0*([0-9]+)\\s*,\\s*0*([0-9]+)\\s*$', passage)[0]\n",
      "        raw_fields = raw_all_fields[1:-2] + [raw_all_fields[-1]]\n",
      "        string_fields = [f.strip('[]') for f in raw_fields]\n",
      "        fields = [f.rsplit('<', 1) for f in string_fields]\n",
      "        new_fields = [(re.sub('\\s*\\+', '', f[0].rstrip()), code_def[f[1].rstrip('>')]) for f in fields]\n",
      "        rules.append([(book_def[p_comp[0]], p_comp[1], p_comp[2]), new_fields[-1], tuple(new_fields[0:-1])])\n",
      "    \n",
      "    for (i, rule) in enumerate(rules):\n",
      "        (passage, target, context) = rule\n",
      "        (target_words, target_code) = target\n",
      "        hits = []\n",
      "        the_hit = -1\n",
      "        for (n, cw) in enumerate(context):\n",
      "            if cw[0] == target_words: hits.append(n)\n",
      "        if not hits:\n",
      "            msg(\"WARNING: Rule {} [{}]: Nothing fits {} in context {}\".format(i+1, passage, target_words, context))\n",
      "        elif len(hits) > 1:\n",
      "            msg(\"WARNING: Rule {} [{}]: More than one phrase fit {} in context {}\".format(i+1, passage, target_words, context))\n",
      "            the_hit = hits[-1]\n",
      "        else: the_hit = hits[-1]\n",
      "        rule.append(the_hit)\n",
      "    return rules"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Retrieving passages"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We examine all the rules, collect the passages that we have to address, and collect data from them from the database. Additionally we ad the node ids of the nodes in each passage for the later generation of the annotation file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_passage(passages):\n",
      "    '''Given a list of passages, look up relevant information for all those passages.\n",
      "    \n",
      "    Do this in one pass over all the nodes.\n",
      "    '''\n",
      "    cur_book = None\n",
      "    cur_chapter = None\n",
      "    cur_passage = None\n",
      "    cur_phrase = [None, []]\n",
      "    verse_phrases = []\n",
      "    \n",
      "    passages_dict = {}\n",
      "    inpassage = False\n",
      "    msg('Look for passages')\n",
      "    for n in NN():\n",
      "        otype = F.otype.v(n)\n",
      "        if otype == 'book':\n",
      "            cur_book = F.book.v(n)\n",
      "        elif otype == 'chapter':\n",
      "            cur_chapter = F.chapter.v(n)\n",
      "        elif otype == 'verse':\n",
      "            cur_verse = F.verse.v(n)\n",
      "            # finish pending phrase\n",
      "            if cur_phrase[0] != None: verse_phrases.append(cur_phrase)  \n",
      "            cur_phrase = [None, []]\n",
      "            # finish pending verse\n",
      "            if verse_phrases:\n",
      "                passages_dict[cur_passage] = {\n",
      "                    'elems': [\n",
      "                        (' '.join([F.surface_consonants.v(y) for y in x[1]]), F.phrase_function.v(x[0]), x[0]) for x in verse_phrases\n",
      "                    ],\n",
      "                    'text': [\n",
      "                        [(F.text.v(y), F.monads.v(y)) for y in x[1]] for x in verse_phrases\n",
      "                    ],\n",
      "                }\n",
      "            verse_phrases = []\n",
      "            if (cur_book, cur_chapter, cur_verse) in passages: \n",
      "                cur_passage = (cur_book, cur_chapter, cur_verse)\n",
      "                inpassage = True\n",
      "                cur_phrase = [None, []]\n",
      "                verse_phrases = []\n",
      "            else: inpassage = False\n",
      "        elif inpassage:\n",
      "            if otype == 'word': cur_phrase[1].append(n)\n",
      "            elif otype == 'phrase':\n",
      "                if cur_phrase[0] != None: verse_phrases.append(cur_phrase)\n",
      "                cur_phrase = [n, []]\n",
      "    # finish pending phrase\n",
      "    if cur_phrase[0] != None: verse_phrases.append(cur_phrase)   \n",
      "    # finish pending verse\n",
      "    if verse_phrases:\n",
      "        passages_dict[cur_passage] = {\n",
      "            'elems': [\n",
      "                (' '.join([F.surface_consonants.v(y) for y in x[1]]), F.phrase_function.v(x[0]), x[0]) for x in verse_phrases\n",
      "            ],\n",
      "            'text': [\n",
      "                [(F.text.v(y), F.monads.v(y)) for y in x[1]] for x in verse_phrases\n",
      "            ],\n",
      "        }\n",
      "    msg(\"End walk\")\n",
      "    return passages_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rules = compile_rules(lines)\n",
      "show = 1\n",
      "rule = rules[show]\n",
      "(passage, target, context, offset) = rule\n",
      "verse = get_passage({passage})[passage]\n",
      "print(\"RULE {}=\\n{}\\nVERSE ELEMS:{}\\nVERSE TEXT={}\".format(show, rule, verse['elems'], verse['text']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    16s Look for passages\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    17s End walk\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RULE 1=\n",
        "[('Exodus', '40', '8'), ('SBJB', 'Cmpl'), (('W', 'Conj'), ('FMT', 'Pred'), ('>T H XYR', 'Objc'), ('SBJB', 'Modi')), 3]\n",
        "VERSE ELEMS:[('W', 'Conj', 636037), ('FMT', 'Pred', 636038), ('>T H XYR', 'Objc', 636039), ('SBJB', 'Modi', 636040), ('W', 'Conj', 636041), ('NTT', 'Pred', 636042), ('>T MSK', 'Objc', 636043), ('C<R H XYR', 'Cmpl', 636044)]\n",
        "VERSE TEXT=[[('\u05d5\u05b0', '52004')], [('\u05e9\u05b7\u05c2\u05de\u05b0\u05ea\u05b8\u05bc\u05a5', '52005')], [('\u05d0\u05b6\u05ea', '52006'), ('\u05d4\u05b6', '52007'), ('\u05d7\u05b8\u05e6\u05b5\u0596\u05e8', '52008')], [('\u05e1\u05b8\u05d1\u05b4\u0591\u05d9\u05d1', '52009')], [('\u05d5\u05b0', '52010')], [('\u05e0\u05b8\u05a3\u05ea\u05b7\u05ea\u05b8\u05bc\u0594', '52011')], [('\u05d0\u05b6\u05ea', '52012'), ('\u05de\u05b8\u05e1\u05b7\u0596\u05da\u05b0', '52013')], [('\u05e9\u05b7\u05c1\u05a5\u05e2\u05b7\u05e8', '52014'), ('\u05d4\u05b6', '52015'), ('\u05d7\u05b8\u05e6\u05b5\u05bd\u05e8', '52016')]]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Match rules with the database"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We compare the context part of each rule with the passage we find in the database. We look for all occurrences of the complete context in the elements of the specified passage.\n",
      "\n",
      "The match_elem function takes care of matching a context against verse material, and after that we run the matching for all rules. The outcome of a match is the list of numbers of the phrases in the verse where the match starts. Particularly interesting are the cases where there is not exactly one match."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def match_elem(context, elements):\n",
      "    matches = []\n",
      "    for (e, elem) in enumerate(elements):\n",
      "        equal = True\n",
      "        for (c, cont) in enumerate(context):\n",
      "            if len(elements) < e + c + 1 or cont[0] != elements[e + c][0]:\n",
      "                equal = False\n",
      "                break\n",
      "        if equal: matches.append(e)\n",
      "    return matches"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "msg(\"Getting the passages material\")\n",
      "elements_dict = get_passage({r[0] for r in rules})\n",
      "msg(\"Done\")\n",
      "annox_list_small = ()\n",
      "data = []\n",
      "\n",
      "for (r, rule) in enumerate(rules):\n",
      "    (passage, target, context, offset) = rule\n",
      "    verse = elements_dict[passage]\n",
      "    verse_elems = verse['elems']\n",
      "    verse_text = verse['text']\n",
      "    matches = match_elem(context, verse_elems)\n",
      "    nm = len(matches)\n",
      "    print('{}: {} matches: {}'.format(passage, nm, matches))\n",
      "    if nm == 0:\n",
      "        print(\"NO MATCH in RULE {}: '{}' versus {}\\n{}\".format(\n",
      "            r, [t[0] for t in context], [\"{} - {}\".format(t[0][0], ','.join([w[1] for w in t[1]])) for t in zip(verse_elems, verse_text)], lines[r],\n",
      "        ))\n",
      "        \n",
      "    cut = []\n",
      "    if len(matches) > 0: \n",
      "        match = matches[0]\n",
      "        cut = verse_elems[match:]\n",
      "        value = cut[offset]\n",
      "        #print(\"MATCH\")\n",
      "        #print(value)\n",
      "        annox_target = rule[1][1]\n",
      "        nodeid = value[2]\n",
      "        xmlid = nodeid\n",
      "        feat = \"phrase_function\"\n",
      "        annox_list_small = (xmlid, annox_target, feat)\n",
      "        data.insert(r, annox_list_small)\n",
      "        annox_list_small= ()\n",
      "        \n",
      "msg(\"Done\")\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    24s Getting the passages material\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    24s Look for passages\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    25s End walk\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    25s Done\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    25s Done\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('Exodus', '15', '25'): 1 matches: [13]\n",
        "('Exodus', '40', '8'): 1 matches: [0]\n",
        "('Leviticus', '6', '3'): 1 matches: [16]\n",
        "('Isaiah', '41', '19'): 0 matches: []\n",
        "NO MATCH in RULE 3: '['>FJM', 'B <RBH', 'BRWC TDHR W T>CWR', 'JXDW']' versus ['>TN - 227281', 'B . MDBR - 227282,227283,227284', '>RZ CVH W HDS W <Y CMN - 227285,227286,227287,227288,227289,227290,227291', '>FJM - 227292', 'B . <RBH - 227293,227294,227295', 'BRWC TDHR W T>CWR - 227296,227297,227298,227299', 'JXDW - 227300']\n",
        "JES 41,19\t[>FJM <Pr>]\t[B <RBH <Lo>]\t[BRWC TDHR W T>CWR <Ob>]\t[JXDW <Mo>]\t:CHANGESTO:\t[B <RBH <Co>]\n",
        "\n",
        "('Isaiah', '42', '4'): 0 matches: []\n",
        "NO MATCH in RULE 4: '['<D', 'JFJM', 'B >RY', 'MCPV']' versus ['L> - 227499', 'JKHH - 227500', 'W - 227501', 'L> - 227502', 'JRWY - 227503', '<D - 227504', 'JFJM - 227505', 'B . >RY - 227506,227507,227508', 'MCPV - 227509', 'W - 227510', 'L TWRTW - 227511,227512', '>JJM - 227513', 'JJXJLW - 227514']\n",
        "JES 42,04\t[<D <Cj>]\t[JFJM <Pr>]\t[B >RY <Lo>]\t[MCPV <Ob>]\t:CHANGESTO:\t[B >RY <Co>]\n",
        "\n",
        "('Isaiah', '43', '19'): 0 matches: []\n",
        "NO MATCH in RULE 5: '['>P', '>FJM', 'B MDBR', 'DRK']' versus ['HNNJ - 228150', '<FH - 228151', 'XDCH - 228152', '<TH - 228153', 'TYMX - 228154', 'H - 228155', 'LW> - 228156', 'TD<WH - 228157', '>P - 228158', '>FJM - 228159', 'B . MDBR - 228160,228161,228162', 'DRK - 228163', 'B JCMWN - 228164,228165', 'NHRWT - 228166']\n",
        "JES 43,19\t[>P <Mo>]\t[>FJM <Pr>]\t[B MDBR <Lo>]\t[DRK <Ob>]\t:CHANGESTO:\t[B MDBR <Co>]\n",
        "\n",
        "('Isaiah', '57', '7'): 1 matches: [0]\n",
        "('Isaiah', '57', '8'): 1 matches: [0]\n",
        "('Ezekiel', '20', '28'): 1 matches: [20]\n",
        "('Habakkuk', '2', '9'): 0 matches: []\n",
        "NO MATCH in RULE 9: '['L FWM', 'B MRWM', 'QNW']' versus ['HWJ - 304435', 'BY< - 304436', 'BY< R< - 304437,304438', 'L BJTW - 304439,304440', 'L FWM - 304441,304442', 'B . MRWM - 304443,304444,304445', 'QNW - 304446', 'L HNYL - 304447,304448', 'M KP R< - 304449,304450,304451']\n",
        "HAB 02,09\t[L FWM <Pr>]\t[B MRWM <Lo>]\t[QN +W <Ob>]\t:CHANGESTO:\t[B MRWM <Co>]\n",
        "\n",
        "('I_Samuel', '2', '20'): 1 matches: [6]\n",
        "('II_Samuel', '14', '7'): 0 matches: []\n",
        "NO MATCH in RULE 11: '['L BLTJ FWM', 'L >JCJ', 'CM W C>RJT', '<L PNJ H >DMH']' versus ['W - 168779', 'HNH - 168780', 'QMH - 168781', 'KL H MCPXH - 168782,168783,168784', '<L CPXTK - 168785,168786', 'W - 168787', 'J>MRW - 168788', 'TNJ - 168789', '>T MKH >XJW - 168790,168791,168792', 'W - 168793', 'NMTHW - 168794', 'B NPC >XJW - 168795,168796,168797', '>CR - 168798', 'HRG - 168799', 'W - 168800', 'NCMJDH - 168801', 'GM - 168802', '>T H JWRC - 168803,168804,168805', 'W - 168806', 'KBW - 168807', '>T GXLTJ - 168808,168809', '>CR - 168810', 'NC>RH - 168811', 'L BLTJ - 168812,168813', 'FWM - 168814', 'L >JCJ - 168815,168816', 'CM W C>RJT - 168817,168818,168819', '<L PNJ H >DMH - 168820,168821,168822,168823']\n",
        "IISA 14,07\t[L BLTJ FWM <Pr>]\t[L >JC +J <sc>]\t[CM W C>RJT <Ob>]\t[<L PNJ H >DMH <Co>]\t:CHANGESTO:\t[L >JC +J <Aj>]\n",
        "\n",
        "('I_Kings', '20', '34'): 1 matches: [9]\n",
        "('II_Kings', '4', '10'): 1 matches: [3]\n",
        "('Isaiah', '21', '4'): 1 matches: [4]\n",
        "('I_Samuel', '19', '13'): 1 matches: [7]\n",
        "('II_Kings', '10', '8'): 1 matches: [11]\n",
        "('II_Kings', '13', '7'): 0 matches: []\n",
        "NO MATCH in RULE 17: '['W', 'JFMM', 'K <PR']' versus ['KJ - 204042', 'L> - 204043', 'HC>JR - 204044', 'L JHW>XZ - 204045,204046', '<M - 204047', 'KJ >M - 204048,204049', 'XMCJM PRCJM W <FRH RKB W <FRT >LPJM RGLJ - 204050,204051,204052,204053,204054,204055,204056,204057,204058', 'KJ - 204059', '>BDM - 204060', 'MLK >RM - 204061,204062', 'W - 204063', 'JFMM - 204064', 'K . <PR - 204065,204066,204067', 'L DC - 204068,204069']\n",
        "IIKON13,07\t[W <Cj>]\t[JFM +M <PO>]\t[K <PR <Aj>]\t:CHANGESTO:\t[K <PR <Co>]\n",
        "\n",
        "('Isaiah', '53', '10'): 1 matches: [5]\n",
        "('Jeremiah', '11', '13'): 0 matches: []\n",
        "NO MATCH in RULE 19: '['W', 'MSPR XYWT JRWCLM', 'FMTM', 'MZBXWT', 'L BCT']' versus ['KJ - 241612', 'MSPR <RJK - 241613,241614', 'HJW - 241615', '>LHJK - 241616', 'JHWDH - 241617', 'W - 241618', 'MSPR XYWT JRWCLM - 241619,241620,241621', 'FMTM - 241622', 'MZBXWT - 241623', 'L . BCT - 241624,241625,241626', 'MZBXWT - 241627', 'L QVR - 241628,241629', 'L . B<L - 241630,241631,241632']\n",
        "JER 11,13\t[W <Cj>]\t[MSPR XYWT JRWCLM <Su>]\t[FMTM <Pr>]\t[MZBXWT <Ob>]\t[L BCT <Co>]\t:CHANGESTO:\t[MSPR XYWT JRWCLM <Ob>]\n",
        "\n",
        "('Ezekiel', '17', '5'): 1 matches: [8]\n",
        "('Ezekiel', '19', '5'): 1 matches: [10]\n",
        "('Hosea', '2', '5'): 0 matches: []\n",
        "NO MATCH in RULE 22: '['W', 'FMTJH', 'K MDBR']' versus ['PN - 292357', '>PCJVNH - 292358', '<RMH - 292359', 'W - 292360', 'HYGTJH - 292361', 'K JWM - 292362,292363', 'HWLDH - 292364', 'W - 292365', 'FMTJH - 292366', 'K . MDBR - 292367,292368,292369', 'W - 292370', 'CTH - 292371', 'K >RY YJH - 292372,292373,292374', 'W - 292375', 'HMTJH - 292376', 'B . YM> - 292377,292378,292379']\n",
        "HOS 02,05\t[W <Cj>]\t[FMTJ +H <PO>]\t[K MDBR <Aj>]\t:CHANGESTO:\t[K MDBR <Co>]\n",
        "\n",
        "('Micah', '1', '7'): 1 matches: [7]\n",
        "('Micah', '2', '12'): 0 matches: []\n",
        "NO MATCH in RULE 24: '['JXD', '>FJMNW', 'K Y>N BYRH']' versus ['>SP - 301667', '>>SP - 301668', 'J<QB - 301669', 'KLK - 301670', 'QBY - 301671', '>QBY - 301672', 'C>RJT JFR>L - 301673,301674', 'JXD - 301675', '>FJMNW - 301676', 'K Y>N - 301677,301678', 'BYRH - 301679', 'K <DR - 301680,301681', 'B TWK H DBRW - 301682,301683,301684,301685', 'THJMNH - 301686', 'M >DM - 301687,301688']\n",
        "MICH02,12\t[JXD <Mo>]\t[>FJMN +W <PO>]\t[K Y>N BYRH <Aj>]\t:CHANGESTO:\t[K Y>N BYRH <Co>]\n",
        "\n",
        "('Psalms', '89', '30'): 1 matches: [0]\n",
        "('Job', '38', '9'): 1 matches: [0]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Creating an annotation file"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With the function create_annots we generate an annotation file for the namespace jj which assignes the corrected values from the code above to the according node in the LAF-resource. The annotation file is an XML-file which can be added to the existing LAF-resourse without the neccessity to re-compile the whole corpus and to this way correct parsing errors in WIVU corpus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_annots(API, data):\n",
      "    \n",
      "    result = []\n",
      "    result.append('''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<graph xmlns=\"http://www.xces.org/ns/GrAF/1.0/\" xmlns:graf=\"http://www.xces.org/ns/GrAF/1.0/\">\n",
      "<graphHeader>\n",
      "    <labelsDecl/>\n",
      "    <dependencies/>\n",
      "    <annotationSpaces/>\n",
      "</graphHeader>''')\n",
      "    aid = 0\n",
      "    features = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(lambda: {})))\n",
      "    \n",
      "    for line in data:\n",
      "        node = line[0]\n",
      "        value = line[1]\n",
      "        feat = line[2]\n",
      "        if len(line) > 3:\n",
      "            feat = \"{}.{}\".format(line[1], feat)\n",
      "        if len(line) > 4:\n",
      "            feat = \"{}:{}\".format(line[2], feat)\n",
      "        (aspace, alabel, fname) = API['fabric'].resolve_feature('node', feat)\n",
      "        xml_id = X.r(node)\n",
      "        aspace = \"jj\"\n",
      "        features[aspace][alabel][xml_id][fname] = value\n",
      "        \n",
      "    for aspace in features:\n",
      "        for alabel in features[aspace]:\n",
      "            for xml_id in features[aspace][alabel]:\n",
      "                aid += 1\n",
      "                result.append('''<a xml:id=\"a{}\" as=\"{}\" label=\"{}\" ref=\"{}\"><fs>'''.format(aid, aspace, alabel, xml_id))\n",
      "                for fname in features[aspace][alabel][xml_id]:\n",
      "                    value = features[aspace][alabel][xml_id][fname]\n",
      "                    result.append('\\t<f name=\"{}\" value=\"{}\"/>'.format(fname, value))\n",
      "                result.append('</fs></a>')\n",
      "    result.append(\"</graph>\")\n",
      "    return '\\n'.join(result)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "annots = create_annots(API, data)\n",
      "\n",
      "print(annots)\n",
      "\n",
      "fobj = open(\"/Users/judith/laf-fabric-data/bhs3.txt.hdr/annotations/dataprep/annots_judith.xml\", \"w\") \n",
      "\n",
      "fobj.write(annots) \n",
      "fobj.close()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<graph xmlns=\"http://www.xces.org/ns/GrAF/1.0/\" xmlns:graf=\"http://www.xces.org/ns/GrAF/1.0/\">\n",
        "<graphHeader>\n",
        "    <labelsDecl/>\n",
        "    <dependencies/>\n",
        "    <annotationSpaces/>\n",
        "</graphHeader>\n",
        "<a xml:id=\"a1\" as=\"jj\" label=\"ft\" ref=\"n932314\"><fs>\n",
        "\t<f name=\"phrase_function\" value=\"Objc\"/>\n",
        "</fs></a>\n",
        "<a xml:id=\"a2\" as=\"jj\" label=\"ft\" ref=\"n209203\"><fs>\n",
        "\t<f name=\"phrase_function\" value=\"Cmpl\"/>\n",
        "</fs></a>\n",
        "<a xml:id=\"a3\" as=\"jj\" label=\"ft\" ref=\"n1000148\"><fs>\n",
        "\t<f name=\"phrase_function\" value=\"Objc\"/>\n",
        "</fs></a>\n",
        "<a xml:id=\"a4\" as=\"jj\" label=\"ft\" ref=\"n1171627\"><fs>\n",
        "\t<f name=\"phrase_function\" value=\"PreS\"/>\n",
        "</fs></a>\n",
        "<a xml:id=\"a5\" as=\"jj\" label=\"ft\" ref=\"n931519\"><fs>\n",
        "\t<f name=\"phrase_function\" value=\"Objc\"/>\n",
        "</fs></a>\n",
        "<a xml:id=\"a6\" as=\"jj\" label=\"ft\" ref=\"n157279\"><fs>\n",
        "\t<f name=\"phrase_function\" value=\"Cmpl\"/>\n",
        "</fs></a>\n",
        "<a xml:id=\"a7\" as=\"jj\" label=\"ft\" ref=\"n673384\"><fs>\n",
        "\t<f name=\"phrase_function\" value=\"Objc\"/>\n",
        "</fs></a>\n",
        "<a xml:id=\"a8\" as=\"jj\" label=\"ft\" ref=\"n497714\"><fs>\n",
        "\t<f name=\"phrase_function\" value=\"Adju\"/>\n",
        "</fs></a>\n",
        "<a xml:id=\"a9\" as=\"jj\" label=\"ft\" ref=\"n149427\"><fs>\n",
        "\t<f name=\"phrase_function\" value=\"Cmpl\"/>\n",
        "</fs></a>\n",
        "<a xml:id=\"a10\" as=\"jj\" label=\"ft\" ref=\"n670313\"><fs>\n",
        "\t<f name=\"phrase_function\" value=\"Adju\"/>\n",
        "</fs></a>\n",
        "<a xml:id=\"a11\" as=\"jj\" label=\"ft\" ref=\"n1105969\"><fs>\n",
        "\t<f name=\"phrase_function\" value=\"Time\"/>\n",
        "</fs></a>\n",
        "<a xml:id=\"a12\" as=\"jj\" label=\"ft\" ref=\"n620677\"><fs>\n",
        "\t<f name=\"phrase_function\" value=\"Adju\"/>\n",
        "</fs></a>\n",
        "<a xml:id=\"a13\" as=\"jj\" label=\"ft\" ref=\"n504507\"><fs>\n",
        "\t<f name=\"phrase_function\" value=\"Objc\"/>\n",
        "</fs></a>\n",
        "<a xml:id=\"a14\" as=\"jj\" label=\"ft\" ref=\"n753891\"><fs>\n",
        "\t<f name=\"phrase_function\" value=\"Cmpl\"/>\n",
        "</fs></a>\n",
        "<a xml:id=\"a15\" as=\"jj\" label=\"ft\" ref=\"n753900\"><fs>\n",
        "\t<f name=\"phrase_function\" value=\"Cmpl\"/>\n",
        "</fs></a>\n",
        "<a xml:id=\"a16\" as=\"jj\" label=\"ft\" ref=\"n932843\"><fs>\n",
        "\t<f name=\"phrase_function\" value=\"Cmpl\"/>\n",
        "</fs></a>\n",
        "<a xml:id=\"a17\" as=\"jj\" label=\"ft\" ref=\"n745089\"><fs>\n",
        "\t<f name=\"phrase_function\" value=\"Adju\"/>\n",
        "</fs></a>\n",
        "<a xml:id=\"a18\" as=\"jj\" label=\"ft\" ref=\"n753242\"><fs>\n",
        "\t<f name=\"phrase_function\" value=\"Objc\"/>\n",
        "</fs></a>\n",
        "</graph>\n"
       ]
      },
      {
       "ename": "FileNotFoundError",
       "evalue": "[Errno 2] No such file or directory: '/Users/judith/laf-fabric-data/bhs3.txt.hdr/annotations/dataprep/annots_judith.xml'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-11-47b402d1d8da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/judith/laf-fabric-data/bhs3.txt.hdr/annotations/dataprep/annots_judith.xml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/judith/laf-fabric-data/bhs3.txt.hdr/annotations/dataprep/annots_judith.xml'"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Check function: Applying the new annotation file and check it agains the corpus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the new fabric.load the LAF-resourse as loaded again together with the newly created annotation file. The check function below checkts the application of the newly added annotation file and shows that the corrections have been added successfully. This way the tool in this notebook can be used for an automatic correction of the LAF-recourse of the WIVU database."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fabric.load('bhs3', 'dataprep', 'dataprep', {\n",
      "    \"primary\": True,\n",
      "    \"xmlids\": {\n",
      "        \"node\": True,\n",
      "        \"edge\": False,\n",
      "    },\n",
      "    \"features\": {\n",
      "        \"shebanq\": {\n",
      "            \"node\": [\n",
      "                \"db.otype,monads\",\n",
      "                \"ft.text,surface_consonants\",\n",
      "                \"ft.phrase_function,locative\",\n",
      "                \"sft.book,chapter,verse_label,verse\",\n",
      "            ],\n",
      "            \"edge\": [\n",
      "            ],\n",
      "        },\n",
      "        \"jj\": {\n",
      "            \"node\": [\n",
      "                \"ft.phrase_function\",\n",
      "            ],\n",
      "        }\n",
      "    },\n",
      "})\n",
      "    \n",
      "exec(fabric.localnames.format(var='fabric'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s LOADING API: please wait ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING m: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s INFO: DATA COMPILED AT: 2014-04-18T18-24-37\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s BEGIN COMPILE a: dataprep\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: load main: X. [node]  -> \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.41s DETAIL: load main: X. [e]  -> \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.89s DETAIL: load main: G.node_anchor_min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.00s DETAIL: load main: G.node_anchor_max\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.10s DETAIL: load main: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.19s DETAIL: load main: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.73s DETAIL: load main: G.edges_from\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.82s DETAIL: load main: G.edges_to\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.93s LOGFILE=/Users/judith/laf-fabric-data/bhs3/bin/A/dataprep/__log__compile__.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.93s PARSING ANNOTATION FILES\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.93s INFO: parsing annots_judith.xml\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.94s INFO: END PARSING\n",
        "         0 good   regions  and     0 faulty ones\n",
        "         0 linked nodes    and     0 unlinked ones\n",
        "         0 good   edges    and     0 faulty ones\n",
        "        18 good   annots   and     0 faulty ones\n",
        "        18 good   features and     0 faulty ones\n",
        "        18 distinct xml identifiers\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.94s MODELING RESULT FILES\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.94s INFO: CONNECTIVITY\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.05s WRITING RESULT FILES for a\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.05s DETAIL: write annox: F.jj_ft_phrase_function [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.06s END   COMPILE a: dataprep\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.06s INFO: DATA COMPILED AT: 2014-06-10T13-10-32\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.07s DETAIL: keep main: G.node_anchor_min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.07s DETAIL: keep main: G.node_anchor_max\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.07s DETAIL: keep main: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.07s DETAIL: keep main: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.07s DETAIL: keep main: G.edges_from\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.07s DETAIL: keep main: G.edges_to\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.07s DETAIL: keep main: X. [node]  -> \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.07s DETAIL: keep main: X. [node]  <- \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.07s DETAIL: keep main: F.shebanq_db_monads [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.07s DETAIL: keep main: F.shebanq_db_otype [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.07s DETAIL: keep main: F.shebanq_ft_locative [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.07s DETAIL: keep main: F.shebanq_ft_phrase_function [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.07s DETAIL: keep main: F.shebanq_ft_surface_consonants [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.07s DETAIL: keep main: F.shebanq_ft_text [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.07s DETAIL: keep main: F.shebanq_sft_book [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.07s DETAIL: keep main: F.shebanq_sft_chapter [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.07s DETAIL: keep main: F.shebanq_sft_verse [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.07s DETAIL: keep main: F.shebanq_sft_verse_label [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.07s DETAIL: clear main: X. [e]  -> \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.07s DETAIL: clear main: X. [e]  <- \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.07s DETAIL: load main: P.node_anchor\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.18s DETAIL: load main: P.node_anchor_items\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.61s DETAIL: load main: P.node_events\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.78s DETAIL: load main: P.node_events_items\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.31s DETAIL: load main: P.node_events_k\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.47s DETAIL: load main: P.node_events_n\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.77s DETAIL: load main: P.primary_data\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.84s DETAIL: load main: F.jj_ft_phrase_function [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.85s DETAIL: load annox: F.jj_ft_phrase_function [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.85s DETAIL: load annox: F.shebanq_db_monads [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.85s DETAIL: load annox: F.shebanq_db_otype [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.85s DETAIL: load annox: F.shebanq_ft_locative [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.85s DETAIL: load annox: F.shebanq_ft_phrase_function [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.85s DETAIL: load annox: F.shebanq_ft_surface_consonants [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.85s DETAIL: load annox: F.shebanq_ft_text [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.85s DETAIL: load annox: F.shebanq_sft_book [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.85s DETAIL: load annox: F.shebanq_sft_chapter [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.85s DETAIL: load annox: F.shebanq_sft_verse [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.85s DETAIL: load annox: F.shebanq_sft_verse_label [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.85s LOGFILE=/Users/judith/laf-fabric-data/bhs3/tasks/dataprep/__log__dataprep.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.85s INFO: Feature ft_phrase_function refers to shebanq_ft_phrase_function, not to jj_ft_phrase_function\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.85s INFO: Feature phrase_function refers to shebanq_ft_phrase_function, not to jj_ft_phrase_function\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.85s INFO: DATA LOADED FROM SOURCE bhs3 AND ANNOX dataprep FOR TASK dataprep\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for element in NN():\n",
      "    jjphrasefunc = F.jj_ft_phrase_function.v(element)\n",
      "    if jjphrasefunc:\n",
      "        otype = otype = F.shebanq_db_otype.v(element)\n",
      "        xml_id = X.r(element)\n",
      "        valueshebanq = F.shebanq_ft_phrase_function.v(element)\n",
      "        print(\"xml_id={} value jj correction={} value shebanq={} otype={} element={}\".format(xml_id, jjphrasefunc, valueshebanq, otype, element))           "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "xml_id=n149427 value jj correction=Cmpl value shebanq=Loca otype=phrase element=628188\n",
        "xml_id=n157279 value jj correction=Cmpl value shebanq=Modi otype=phrase element=636040\n",
        "xml_id=n209203 value jj correction=Cmpl value shebanq=Adju otype=phrase element=637690\n",
        "xml_id=n497714 value jj correction=Adju value shebanq=Supp otype=phrase element=685929"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "xml_id=n504507 value jj correction=Objc value shebanq=Cmpl otype=phrase element=692722\n",
        "xml_id=n620677 value jj correction=Adju value shebanq=Supp otype=phrase element=716595"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "xml_id=n670313 value jj correction=Adju value shebanq=Supp otype=phrase element=719185\n",
        "xml_id=n673384 value jj correction=Objc value shebanq=Adju otype=phrase element=722256\n",
        "xml_id=n745089 value jj correction=Adju value shebanq=Supp otype=phrase element=732141"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "xml_id=n753242 value jj correction=Objc value shebanq=Subj otype=phrase element=740294\n",
        "xml_id=n753891 value jj correction=Cmpl value shebanq=Loca otype=phrase element=740943\n",
        "xml_id=n753900 value jj correction=Cmpl value shebanq=Loca otype=phrase element=740952\n",
        "xml_id=n931519 value jj correction=Objc value shebanq=Unkn otype=phrase element=765729"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "xml_id=n932314 value jj correction=Objc value shebanq=Unkn otype=phrase element=766524\n",
        "xml_id=n932843 value jj correction=Cmpl value shebanq=Modi otype=phrase element=767053\n",
        "xml_id=n1000148 value jj correction=Objc value shebanq=Unkn otype=phrase element=783184"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "xml_id=n1105969 value jj correction=Time value shebanq=Unkn otype=phrase element=801346"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "xml_id=n1171627 value jj correction=PreS value shebanq=PreO otype=phrase element=815739"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}