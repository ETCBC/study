{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import collections\n",
      "\n",
      "from laf.fabric import LafFabric\n",
      "fabric = LafFabric()\n",
      "from etcbc.preprocess import prepare"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s This is LAF-Fabric 4.0.3\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fabric.load('bhs3.txt.hdr', '--', 'dataprep',\n",
      "        {\n",
      "            \"xmlids\": {\n",
      "                \"node\": False,\n",
      "                \"edge\": False,\n",
      "            },\n",
      "            \"features\": {\n",
      "                \"shebanq\": {\n",
      "                    \"node\": [\n",
      "                        \"db.otype\",\n",
      "                        \"ft.surface_consonants\",\n",
      "                        \"ft.phrase_function,locative\",\n",
      "                        \"sft.book,chapter,verse_label,verse\",\n",
      "                    ],\n",
      "                    \"edge\": [\n",
      "                    ],\n",
      "                },\n",
      "            },\n",
      "            'prepare': prepare,\n",
      "        },\n",
      "        compile_main=False, compile_annox=False,\n",
      "        verbose='DETAIL',\n",
      "    )\n",
      "\n",
      "exec(fabric.localnames.format(var='fabric'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s LOADING API: please wait ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING m: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING a: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: load main: G.node_anchor_min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
<<<<<<< HEAD
        "  0.11s DETAIL: load main: G.node_anchor_max\n"
=======
        "  0.14s DETAIL: load main: G.node_anchor_max\n"
>>>>>>> 74103a3c2d8ed94b1a788798aba96f1381b8825a
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
<<<<<<< HEAD
        "  0.20s DETAIL: load main: G.node_sort\n"
=======
        "  0.27s DETAIL: load main: G.node_sort\n"
>>>>>>> 74103a3c2d8ed94b1a788798aba96f1381b8825a
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
<<<<<<< HEAD
        "  0.29s DETAIL: load main: G.node_sort_inv\n"
=======
        "  0.40s DETAIL: load main: G.node_sort_inv\n"
>>>>>>> 74103a3c2d8ed94b1a788798aba96f1381b8825a
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
<<<<<<< HEAD
        "  0.78s DETAIL: load main: G.edges_from\n"
=======
        "  1.09s DETAIL: load main: G.edges_from\n"
>>>>>>> 74103a3c2d8ed94b1a788798aba96f1381b8825a
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
<<<<<<< HEAD
        "  0.86s DETAIL: load main: G.edges_to\n"
=======
        "  1.20s DETAIL: load main: G.edges_to\n"
>>>>>>> 74103a3c2d8ed94b1a788798aba96f1381b8825a
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
<<<<<<< HEAD
        "  0.95s DETAIL: load main: F.shebanq_db_otype [node] \n"
=======
        "  1.34s DETAIL: load main: F.shebanq_db_otype [node] \n"
>>>>>>> 74103a3c2d8ed94b1a788798aba96f1381b8825a
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
<<<<<<< HEAD
        "  1.63s DETAIL: load main: F.shebanq_ft_surface_consonants [node] \n"
=======
        "  2.28s DETAIL: load main: F.shebanq_ft_surface_consonants [node] \n"
>>>>>>> 74103a3c2d8ed94b1a788798aba96f1381b8825a
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
<<<<<<< HEAD
        "  1.84s DETAIL: load main: F.shebanq_ft_phrase_function [node] \n"
=======
        "  2.58s DETAIL: load main: F.shebanq_ft_phrase_function [node] \n"
>>>>>>> 74103a3c2d8ed94b1a788798aba96f1381b8825a
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
<<<<<<< HEAD
        "  1.96s DETAIL: load main: F.shebanq_ft_locative [node] \n"
=======
        "  2.75s DETAIL: load main: F.shebanq_ft_locative [node] \n"
>>>>>>> 74103a3c2d8ed94b1a788798aba96f1381b8825a
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
<<<<<<< HEAD
        "  2.15s DETAIL: load main: F.shebanq_sft_book [node] \n"
=======
        "  3.03s DETAIL: load main: F.shebanq_sft_book [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.05s DETAIL: load main: F.shebanq_sft_chapter [node] \n"
>>>>>>> 74103a3c2d8ed94b1a788798aba96f1381b8825a
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.16s DETAIL: load main: F.shebanq_sft_verse_label [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.18s DETAIL: load main: F.shebanq_sft_verse [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.19s LOGFILE=/Users/judith/laf-fabric-data/etcbc-bhs3/tasks/bhs3.txt.hdr/dataprep/__log__dataprep.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
<<<<<<< HEAD
        "  2.19s DETAIL: prep prep: G.node_sort\n"
=======
        "  3.12s DETAIL: prep prep: G.node_sort\n"
>>>>>>> 74103a3c2d8ed94b1a788798aba96f1381b8825a
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
<<<<<<< HEAD
        "  2.29s DETAIL: prep prep: G.node_sort_inv\n"
=======
        "  3.25s DETAIL: prep prep: G.node_sort_inv\n"
>>>>>>> 74103a3c2d8ed94b1a788798aba96f1381b8825a
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.81s INFO: DATA LOADED FROM SOURCE bhs3.txt.hdr AND ANNOX -- FOR TASK dataprep\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Specification of steps ahead\n",
      "\n",
      "1. In the file, find out which of the phrases before :CHANGESTO: has to receive a modified phrase function. This is the target phrase.\n",
      "2. Match the target phrase with a phrase occurrence in the indicated passage in the database.\n",
      "3. Produce an annoting input file, tab separated, 2 columns: the oid of the target phrase, the modified phrase_function.\n",
      "   The name of this column should be shebanq:ft.phrase_function\n",
      "4. Use the module etcbc.annotating (and the method make_form to produce a file of annotations\n",
      "5. put the file of annotations in a new annox (in the annotations directory)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Implementing Flow Charts\n",
      "\n",
      "1. Pick one or two flow charts by Janet.\n",
      "2. translate them in a model.\n",
      "3. Use the model to analyse relevant occurrences of phrases with verbs\n",
      "4. Produce the output of the analysis.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "handle = open('CorrectionsFJM.txt', encoding=\"utf8\")\n",
      "lines = [line for line in handle]\n",
      "handle.close()"
     ],
     "language": "python",
     "metadata": {},
<<<<<<< HEAD
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.85s phrase (737651)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.85s word (227161)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.85s phrase (737652)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.85s word (227162)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.85s phrase (737653)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.85s word (227163)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.85s phrase (737654)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.85s word (227164)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.85s phrase (737655)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.85s word (227165)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.85s phrase (737656)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.85s word (227166)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.86s phrase (737657)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.86s word (227167)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.86s phrase (737658)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.86s word (227168)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.86s phrase (737659)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.86s word (227169)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.86s phrase (737660)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.86s word (227170)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.86s phrase (737661)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.86s word (227171)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.86s word (227173)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.86s word (227172)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.86s phrase (737662)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.86s word (227174)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.86s phrase (737663)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.86s word (227175)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.86s word (227176)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.86s phrase (737664)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.86s word (227177)\n"
       ]
      }
     ],
=======
     "outputs": [],
>>>>>>> 74103a3c2d8ed94b1a788798aba96f1381b8825a
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "code_def = {\n",
      " '..': '..',\n",
      " 'Aj': 'Adju',\n",
      " 'Cj': 'Conj',\n",
      " 'Co': 'Cmpl',\n",
      " 'Lo': 'Loca',\n",
      " 'Mo': 'Modi',\n",
      " 'Ob': 'Objc',\n",
      " 'PO': 'PreO',\n",
      " 'Pr': 'Pred',\n",
      " 'Ps': 'PreS',\n",
      " 'Su': 'Subj',\n",
      " 'Ti': 'Time',\n",
      " 'sc': 'Supp',\n",
      "}\n",
      "\n",
      "book_def = {\n",
      "    'GEN': 'Genesis',\n",
      "    'EXO': 'Exodus',\n",
      "    'LEV': 'Leviticus',\n",
      "    'NUM': 'Numbers',\n",
      "    'DEU': 'Deuteronomy',\n",
      "    'JOS': 'Joshua',\n",
      "    'JUD': 'Judges',\n",
      "    'ISAM': 'I_Samuel',\n",
      "    'IISA': 'II_Samuel',\n",
      "    'IKON': 'I_Kings',\n",
      "    'IIKON': 'II_Kings',\n",
      "    'JES': 'Isaiah',\n",
      "    'JER': 'Jeremiah',\n",
      "    'EZE': 'Ezekiel',\n",
      "    'HOS': 'Hosea',\n",
      "    'JOE': 'Joel',\n",
      "    'AMO': 'Amos',\n",
      "    'OBA': 'Obadiah',\n",
      "    'JON': 'Jonah',\n",
      "    'MICH': 'Micah',\n",
      "    'NAH': 'Nahum',\n",
      "    'HAB': 'Habakkuk',\n",
      "    'ZEP': 'Zephaniah',\n",
      "    'HAG': 'Haggai',\n",
      "    'ZEC': 'Zechariah',\n",
      "    'MAL': 'Malachi',\n",
      "    'PS': 'Psalms',\n",
      "    'IOB': 'Job',\n",
      "    'PRO': 'Proverbs',\n",
      "    'RUT': 'Ruth',\n",
      "    'CAN': 'Canticles',\n",
      "    'ECC': 'Ecclesiastes',\n",
      "    'LAM': 'Lamentations',\n",
      "    'EST': 'Esther',\n",
      "    'DAN': 'Daniel',\n",
      "    'EZR': 'Ezra',\n",
      "    'NEH': 'Nehemiah',\n",
      "    'I_c': 'I_Chronicles',\n",
      "    'Ii_': 'II_Chronicles',    \n",
      "}\n",
      "\n",
      "def compile_rules(lines):\n",
      "    rules = []\n",
      "    for line in lines:\n",
      "        line = line.strip()\n",
      "        raw_all_fields = line.split('\\t')\n",
      "        passage = raw_all_fields[0]\n",
      "        p_comp = re.findall('^([A-Z]+)\\s*0*([0-9]+)\\s*,\\s*0*([0-9]+)\\s*$', passage)[0]\n",
      "        raw_fields = raw_all_fields[1:-2] + [raw_all_fields[-1]]\n",
      "        string_fields = [f.strip('[]') for f in raw_fields]\n",
      "        fields = [f.rsplit('<', 1) for f in string_fields]\n",
      "        new_fields = [(re.sub('\\s*\\+', '', f[0].rstrip()), code_def[f[1].rstrip('>')]) for f in fields]\n",
      "        rules.append([(book_def[p_comp[0]], p_comp[1], p_comp[2]), new_fields[-1], tuple(new_fields[0:-1])])\n",
      "    \n",
      "    #for rule in rules:\n",
      "    #    passages.add(rule[0])\n",
      "    \n",
      "    for (i, rule) in enumerate(rules):\n",
      "        passage = rule[0]\n",
      "        (target_words, target_code) = rule[1]\n",
      "        context = rule[2]\n",
      "        hits = []\n",
      "        the_hit = -1\n",
      "        for (n, cw) in enumerate(context):\n",
      "            if cw[0] == target_words: hits.append(n)\n",
      "        if not hits:\n",
      "            msg(\"WARNING: Rule {} [{}]: Nothing fits {} in context {}\".format(i+1, passage, target_words, context))\n",
      "        elif len(hits) > 1:\n",
      "            msg(\"WARNING: Rule {} [{}]: More than one phrase fit {} in context {}\".format(i+1, passage, target_words, context))\n",
      "            the_hit = hits[-1]\n",
      "        else: the_hit = hits[-1]\n",
      "        rule.append(the_hit)\n",
      "        return rules\n",
      "    #    print('{}\\n'.format(rule))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_passage(passages):\n",
      "    cur_book = None\n",
      "    cur_chapter = None\n",
      "    cur_passage = None\n",
      "    cur_phrase = [None, []]\n",
      "    verse_phrases = []\n",
      "    \n",
      "    passages_dict = {}\n",
      "    inpassage = False\n",
      "    msg('Look for passages')\n",
      "    for n in NN():\n",
      "        otype = F.otype.v(n)\n",
      "        if otype == 'book':\n",
      "            cur_book = F.book.v(n)\n",
      "        elif otype == 'chapter':\n",
      "            cur_chapter = F.chapter.v(n)\n",
      "        elif otype == 'verse':\n",
      "            cur_verse = F.verse.v(n)\n",
      "            if cur_phrase[0] != None: verse_phrases.append(cur_phrase)\n",
      "            cur_phrase = [None, []]\n",
      "            if verse_phrases:\n",
      "                passages_dict[cur_passage] = [\n",
      "                    (' '.join([F.surface_consonants.v(y) for y in x[1]]), F.phrase_function.v(x[0])) for x in verse_phrases\n",
      "                ]\n",
      "            verse_phrases = []\n",
      "            if (cur_book, cur_chapter, cur_verse) in passages: \n",
      "                cur_passage = (cur_book, cur_chapter, cur_verse)\n",
      "                inpassage = True\n",
      "                cur_phrase = [None, []]\n",
      "                verse_phrases = []\n",
      "            else: inpassage = False\n",
      "        elif inpassage:\n",
      "            if otype == 'word': cur_phrase[1].append(n)\n",
      "            elif otype == 'phrase':\n",
      "                if cur_phrase[0] != None: verse_phrases.append(cur_phrase)\n",
      "                cur_phrase = [n, []]\n",
      "    if cur_phrase[0] != None: verse_phrases.append(cur_phrase)\n",
      "    if verse_phrases:\n",
      "        passages_dict[cur_passage] = [\n",
      "            (' '.join([F.surface_consonants.v(y) for y in x[1]]), F.phrase_function.v(x[0])) for x in verse_phrases\n",
      "        ]\n",
      "    msg(\"End walk\")\n",
      "    return passages_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rules = compile_rules(lines)\n",
      "rule = rules[0]\n",
      "elements = get_passage('Exodus', '15', '25')\n",
      "print(\"RULES=\\n{}\\nELEMENTS={}\\n\".format(rule, elements))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    16s Look for passage\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    16s End walk\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RULES=\n",
        "[('Exodus', '15', '25'), ('CM', 'Cmpl'), (('CM', 'Loca'), ('FM', 'Pred'), ('LW', 'Cmpl'), ('XQ W MCPV', 'Objc')), 0]\n",
        "ELEMENTS=[('W', 'Conj'), ('JY<Q', 'Pred'), ('>L JHWH', 'Cmpl'), ('W', 'Conj'), ('JWRHW', 'PreO'), ('JHWH', 'Subj'), ('<Y', 'Objc'), ('W', 'Conj'), ('JCLK', 'Pred'), ('>L H MJM', 'Cmpl'), ('W', 'Conj'), ('JMTQW', 'Pred'), ('H MJM', 'Subj'), ('CM', 'Loca'), ('FM', 'Pred'), ('LW', 'Cmpl'), ('XQ W MCPV', 'Objc'), ('W', 'Conj'), ('CM', 'Loca'), ('NSHW', 'PreO')]\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
<<<<<<< HEAD
      "handle = open('CorrectionsFJM.txt')\n",
      "\n",
      "oldlable = re.compile('<([^> ]*)>')\n",
      "changematch = re.compile('') \n",
      "rget = [] \n",
      "targetlist = []\n",
      "\n",
      "for line in handle: \n",
      "    #matches = oldlable.findall(line) \n",
      "    #if matches: \n",
      "        #for match in matches: \n",
      "         #   matchset.add(match) \n",
      "    target = line.split('\\t') \n",
      "    targetlist.append(target)\n",
      "    \n",
      "handle.close()\n",
      "    \n",
      "bad_chars = ']['\n",
      "rgx = re.compile('[^%s]' % bad_chars)\n",
      "replacelist = []\n",
      "\n",
      "for element in targetlist: \n",
      "    target = element[-1]\n",
      "    newtarget = rgx.findall(target)\n",
      "    stringtar = ''.join(newtarget)\n",
      "    finaltarget = stringtar.rsplit('<', 1)\n",
      "    needs_to_be_replaced = finaltarget[0]\n",
      "    replacelist.append(needs_to_be_replaced)\n",
      "    \n",
      "targetindex = 0\n",
      "\n",
      "handle = open('CorrectionsFJM.txt')\n",
      "\n",
      "bad_chars = ']['\n",
      "plusw = '\\+$'\n",
      "\n",
      "rgx = re.compile('[^%s]' % bad_chars)\n",
      "rgxw = re.compile('[%s]' % plusw)\n",
      "check = 0\n",
      "chunk = collections.defaultdict(list)\n",
      "positions = []\n",
      "\n",
      "for line in handle:\n",
      "    newline = line.split('\\t')\n",
      "    replacement = replacelist[targetindex]\n",
      "    for element in newline:\n",
      "        if rgxw.search(element):\n",
      "            newelement = element.rsplit(' +',1)\n",
      "            correction = ''.join(newelement)\n",
      "            newline = [w.replace(element, correction) for w in newline]\n",
      "        newelement = rgx.findall(element)\n",
      "        finalelement = ''.join(newelement)\n",
      "        result = finalelement.rsplit('<', 1)\n",
      "        if result[0] == replacement and check < 1:\n",
      "            check += 1\n",
      "            #print(result[0])\n",
      "            target = result[0]\n",
      "    checkline = newline[1:-2]\n",
      "    positions.append(newline[0])\n",
      "    checkline = ' '.join(checkline)\n",
      "    chunk[target].append(checkline)\n",
      "    check = 0\n",
      "    targetindex += 1\n",
      "\n",
      "handle.close()\n",
      "\n",
      "chunk"
=======
      "def match_elem(context, elements):\n",
      "    matches = []\n",
      "    for (e, elem) in enumerate(elements):\n",
      "        equal = True\n",
      "        for (c, cont) in enumerate(context):\n",
      "            if cont[0] != elements[e + c][0]:\n",
      "                equal = False\n",
      "                break\n",
      "        if equal: matches.append(e)\n",
      "    return matches\n",
      "        \n",
      "hits = match_elem(rule[2], elements)\n",
      "hits"
>>>>>>> 74103a3c2d8ed94b1a788798aba96f1381b8825a
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
<<<<<<< HEAD
       "prompt_number": 5,
       "text": [
        "defaultdict(<class 'list'>, {'B FWM +J ': ['[B FWMJ <PO>] [<NN <..>] [LBCW <..>]'], 'L +W ': ['[W <Cj>] [NFJM <Pr>] [LW <sc>] [CM <Co>] [MVH W CLXN W KS> W MNWRH <Ob>]'], 'MR>CTJ +W ': ['[W <Cj>] [>T KBJR H <ZJM <Ob>] [FMH <Pr>] [MR>CTJW <Co>]'], 'L <D ': ['[W <Cj>] [FMTJ <Pr>] [L <D <Co>] [ZR<W <Ob>]'], 'L >JC +J ': ['[L BLTJ FWM <Pr>] [L >JCJ <sc>] [CM W C>RJT <Ob>] [<L PNJ H >DMH <Co>]'], 'B  <RBH ': ['[>FJM <Pr>] [B  <RBH <Lo>] [BRWC TDHR W T>CWR <Ob>] [JXDW <Mo>]'], '>YL H MZBX ': ['[W <Cj>] [FMW <PO>] [>YL H MZBX <Aj>]'], 'KPJR ': ['[KPJR <Co>] [FMTHW <PO>]'], 'B  MRWM ': ['[L FWM <Pr>] [B  MRWM <Lo>]'], 'SBJB ': ['[W <Cj>] [FMT <Pr>] [>T H XYR <Ob>] [SBJB <Mo>]'], 'K Y>N BYRH ': ['[JXD <Mo>] [>FJMNW <PO>] [K Y>N BYRH <Aj>]'], 'CNJ YBRJM ': ['[FJMW <Pr>] [>TM <Ob>] [CNJ YBRJM <Aj>] [PTX H C<R <Co>] [<D H BQR <Ti>]'], 'L +J ': ['[>T NCP XCQJ <Ob>] [FM <Pr>] [LJ <sc>] [L XRDH <Co>]'], '<L HR GBH W NF> ': ['[<L HR GBH W NF> <Lo>] [FMT <Pr>] [MCKBK <Ob>]'], 'K  MDBR ': ['[W <Cj>] [FMTJH <PO>] [K  MDBR <Aj>]'], 'K  <PR ': ['[W <Cj>] [JFMM <PO>]'], 'L +K ': ['[JFM <Pr>] [JHWH <Su>] [LK <sc>] [ZR< MN H >CH H Z>T <Ob>] [TXT H C>LH <Co>]', '[W <Cj>] [XWYWT <Ob>] [TFJM <Pr>] [LK <sc>]'], 'YPYPH ': ['[YPYPH <Co>] [FMW <PO>]'], 'B  >RY ': ['[<D <Cj>] [JFJM <Pr>] [B  >RY <Lo>] [MCPV <Ob>]'], 'CM ': ['[CM <Lo>] [FM <Pr>] [LW <Co>] [XQ W MCPV <Ob>]', '[W <Cj>] [JFJMW <Pr>] [CM <Mo>] [RJX NJXWXJHM <Ob>]'], 'B  MDBR ': ['[>P <Mo>] [>FJM <Pr>] [B  MDBR <Lo>] [DRK <Ob>]'], '>XR H DLT W H MZWZH ': ['[W <Cj>] [>XR H DLT W H MZWZH <Lo>] [FMT <Pr>] [ZKRWNK <Ob>]'], 'CMMH ': ['[W <Cj>] [KL <YBJH <Ob>] [>FJM <Pr>] [CMMH <Co>]'], 'NPC +W ': ['[>M <Cj>] [TFJM <Pr>] [>CM <Ob>] [NPCW <Su>]'], 'MSPR XYWT JRWCLM ': ['[W <Cj>] [MSPR XYWT JRWCLM <Su>] [FMTM <Pr>] [MZBXWT <Ob>] [L  BCT <Co>]']})"
       ]
      }
     ],
     "prompt_number": 5
=======
       "prompt_number": 40,
       "text": [
        "[]"
       ]
      }
     ],
     "prompt_number": 40
>>>>>>> 74103a3c2d8ed94b1a788798aba96f1381b8825a
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "msg(\"Getting the passages material\")\n",
      "elements_dict = get_passage({r[0] for r in rules})\n",
      "msg(\"Done\")\n",
      "\n",
      "for rule in rules:\n",
      "    matches = match_elem(rule[2], elements_dict[rule[0]])\n",
      "    print('{} matches: {}'.format(len(matches), matches))\n",
      "msg(\"Done\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
<<<<<<< HEAD
        " 3m 57s Look for passage\n"
=======
        " 1h 23m 28s Getting the passages material\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1h 23m 28s Look for passages\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1h 23m 30s End walk\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1h 23m 30s Done\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1h 23m 30s Done\n"
>>>>>>> 74103a3c2d8ed94b1a788798aba96f1381b8825a
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 matches: [13]\n",
        "1 matches: [0]\n",
        "1 matches: [16]\n",
        "0 matches: []\n",
        "0 matches: []\n",
        "0 matches: []\n",
        "1 matches: [0]\n",
        "1 matches: [0]\n",
        "1 matches: [20]\n",
        "0 matches: []\n",
        "1 matches: [6]\n",
        "0 matches: []\n",
        "1 matches: [9]\n",
        "1 matches: [3]\n",
        "1 matches: [4]\n",
        "1 matches: [7]\n",
        "1 matches: [11]\n",
        "0 matches: []\n",
        "1 matches: [5]\n",
        "0 matches: []\n",
        "1 matches: [8]\n",
        "1 matches: [10]\n",
        "0 matches: []\n",
        "1 matches: [7]\n",
        "0 matches: []\n",
        "1 matches: [0]\n",
        "1 matches: [0]\n"
       ]
      }
     ],
<<<<<<< HEAD
     "prompt_number": 6
=======
     "prompt_number": 60
>>>>>>> 74103a3c2d8ed94b1a788798aba96f1381b8825a
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}