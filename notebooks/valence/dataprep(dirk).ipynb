{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Towards a valency dictionary"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A valency dictionary is a kind of flow chart by which you can separate out verb meanings based on the number and nature of the complements they take. We want to be able to automatically match contexts against flow charts.\n",
      "\n",
      "The work in this notebook consists of two parts: applying corrections to the data and implementing flow charts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import collections\n",
      "\n",
      "from laf.fabric import LafFabric\n",
      "fabric = LafFabric()\n",
      "from etcbc.preprocess import prepare"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s This is LAF-Fabric 4.0.3\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fabric.load('bhs3.txt.hdr', '--', 'dataprep',\n",
      "        {\n",
      "            \"xmlids\": {\n",
      "                \"node\": False,\n",
      "                \"edge\": False,\n",
      "            },\n",
      "            \"features\": {\n",
      "                \"shebanq\": {\n",
      "                    \"node\": [\n",
      "                        \"db.otype,monads\",\n",
      "                        \"ft.text,surface_consonants\",\n",
      "                        \"ft.phrase_function,locative\",\n",
      "                        \"sft.book,chapter,verse_label,verse\",\n",
      "                    ],\n",
      "                    \"edge\": [\n",
      "                    ],\n",
      "                },\n",
      "            },\n",
      "            'prepare': prepare,\n",
      "        },\n",
      "        compile_main=False, compile_annox=False,\n",
      "        verbose='DETAIL',\n",
      "    )\n",
      "\n",
      "exec(fabric.localnames.format(var='fabric'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s LOADING API: please wait ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING m: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING a: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: load main: G.node_anchor_min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.11s DETAIL: load main: G.node_anchor_max\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.20s DETAIL: load main: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.29s DETAIL: load main: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.75s DETAIL: load main: G.edges_from\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.83s DETAIL: load main: G.edges_to\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.92s DETAIL: load main: F.shebanq_db_otype [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.57s DETAIL: load main: F.shebanq_db_monads [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.39s DETAIL: load main: F.shebanq_ft_text [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.73s DETAIL: load main: F.shebanq_ft_surface_consonants [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.93s DETAIL: load main: F.shebanq_ft_phrase_function [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.05s DETAIL: load main: F.shebanq_ft_locative [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.25s DETAIL: load main: F.shebanq_sft_book [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.26s DETAIL: load main: F.shebanq_sft_chapter [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.27s DETAIL: load main: F.shebanq_sft_verse_label [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.30s DETAIL: load main: F.shebanq_sft_verse [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.31s LOGFILE=/Users/judith/laf-fabric-data/etcbc-bhs3/tasks/bhs3.txt.hdr/dataprep/__log__dataprep.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.31s DETAIL: prep prep: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.41s DETAIL: prep prep: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.92s INFO: DATA LOADED FROM SOURCE bhs3.txt.hdr AND ANNOX -- FOR TASK dataprep\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Applying corrections on the parsing analysis of the data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Janet has files with nearly formal corrections to the parsing analysis that has been coded into the BHS3 version of the Hebrew Text Database.\n",
      "\n",
      "We are going to translate those instructions into new LAF annotations: an annox package.\n",
      "\n",
      "# Specification of steps ahead\n",
      "\n",
      "1. In the corrections file, find out which of the phrases before :CHANGESTO: has to receive a modified phrase function.\n",
      "   This is the target phrase.\n",
      "2. Match the target phrase with a phrase occurrence in the indicated passage in the database.\n",
      "3. Produce an annoting input file, tab separated, 2 columns: the oid of the target phrase, the modified phrase_function.\n",
      "   The name of this column should be shebanq:ft.phrase_function\n",
      "4. Use the module etcbc.annotating (and the method make_form to produce a file of annotations\n",
      "5. put the file of annotations in a new annox (in the annotations directory)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Modeling the correction instructions\n",
      "\n",
      "We model the information in each rule in a datastructure, containing all the bits needed for the rest of the tasks in a readily available form.\n",
      "\n",
      "Have a look at the file with corrections, we show just one line here.\n",
      "We apply some consistency in book names and parsing labels between the ways they are represented in the corrections file and in the database."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "handle = open('CorrectionsFJM.txt', encoding=\"utf8\")\n",
      "lines = [line for line in handle]\n",
      "handle.close()\n",
      "lines[3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "'JES 41,19\\t[>FJM <Pr>]\\t[B <RBH <Lo>]\\t[BRWC TDHR W T>CWR <Ob>]\\t[JXDW <Mo>]\\t:CHANGESTO:\\t[B <RBH <Co>]\\n'"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "code_def = {\n",
      " '..': '..',\n",
      " 'Aj': 'Adju',\n",
      " 'Cj': 'Conj',\n",
      " 'Co': 'Cmpl',\n",
      " 'Lo': 'Loca',\n",
      " 'Mo': 'Modi',\n",
      " 'Ob': 'Objc',\n",
      " 'PO': 'PreO',\n",
      " 'Pr': 'Pred',\n",
      " 'Ps': 'PreS',\n",
      " 'Su': 'Subj',\n",
      " 'Ti': 'Time',\n",
      " 'sc': 'Supp',\n",
      "}\n",
      "\n",
      "book_def = {\n",
      "    'GEN': 'Genesis',\n",
      "    'EXO': 'Exodus',\n",
      "    'LEV': 'Leviticus',\n",
      "    'NUM': 'Numbers',\n",
      "    'DEU': 'Deuteronomy',\n",
      "    'JOS': 'Joshua',\n",
      "    'JUD': 'Judges',\n",
      "    'ISAM': 'I_Samuel',\n",
      "    'IISA': 'II_Samuel',\n",
      "    'IKON': 'I_Kings',\n",
      "    'IIKON': 'II_Kings',\n",
      "    'JES': 'Isaiah',\n",
      "    'JER': 'Jeremiah',\n",
      "    'EZE': 'Ezekiel',\n",
      "    'HOS': 'Hosea',\n",
      "    'JOE': 'Joel',\n",
      "    'AMO': 'Amos',\n",
      "    'OBA': 'Obadiah',\n",
      "    'JON': 'Jonah',\n",
      "    'MICH': 'Micah',\n",
      "    'NAH': 'Nahum',\n",
      "    'HAB': 'Habakkuk',\n",
      "    'ZEP': 'Zephaniah',\n",
      "    'HAG': 'Haggai',\n",
      "    'ZEC': 'Zechariah',\n",
      "    'MAL': 'Malachi',\n",
      "    'PS': 'Psalms',\n",
      "    'IOB': 'Job',\n",
      "    'PRO': 'Proverbs',\n",
      "    'RUT': 'Ruth',\n",
      "    'CAN': 'Canticles',\n",
      "    'ECC': 'Ecclesiastes',\n",
      "    'LAM': 'Lamentations',\n",
      "    'EST': 'Esther',\n",
      "    'DAN': 'Daniel',\n",
      "    'EZR': 'Ezra',\n",
      "    'NEH': 'Nehemiah',\n",
      "    'I_c': 'I_Chronicles',\n",
      "    'Ii_': 'II_Chronicles',    \n",
      "}\n",
      "\n",
      "def compile_rules(lines):\n",
      "    rules = []\n",
      "    for line in lines:\n",
      "        line = line.strip()\n",
      "        raw_all_fields = line.split('\\t')\n",
      "        passage = raw_all_fields[0]\n",
      "        p_comp = re.findall('^([A-Z]+)\\s*0*([0-9]+)\\s*,\\s*0*([0-9]+)\\s*$', passage)[0]\n",
      "        raw_fields = raw_all_fields[1:-2] + [raw_all_fields[-1]]\n",
      "        string_fields = [f.strip('[]') for f in raw_fields]\n",
      "        fields = [f.rsplit('<', 1) for f in string_fields]\n",
      "        new_fields = [(re.sub('\\s*\\+', '', f[0].rstrip()), code_def[f[1].rstrip('>')]) for f in fields]\n",
      "        rules.append([(book_def[p_comp[0]], p_comp[1], p_comp[2]), new_fields[-1], tuple(new_fields[0:-1])])\n",
      "    \n",
      "    for (i, rule) in enumerate(rules):\n",
      "        (passage, target, context) = rule\n",
      "        (target_words, target_code) = target\n",
      "        hits = []\n",
      "        the_hit = -1\n",
      "        for (n, cw) in enumerate(context):\n",
      "            if cw[0] == target_words: hits.append(n)\n",
      "        if not hits:\n",
      "            msg(\"WARNING: Rule {} [{}]: Nothing fits {} in context {}\".format(i+1, passage, target_words, context))\n",
      "        elif len(hits) > 1:\n",
      "            msg(\"WARNING: Rule {} [{}]: More than one phrase fit {} in context {}\".format(i+1, passage, target_words, context))\n",
      "            the_hit = hits[-1]\n",
      "        else: the_hit = hits[-1]\n",
      "        rule.append(the_hit)\n",
      "    return rules"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Retrieving passages\n",
      "\n",
      "We examine all the rules, collect the passages that we have to address, and collect data from them from the database."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_passage(passages):\n",
      "    cur_book = None\n",
      "    cur_chapter = None\n",
      "    cur_passage = None\n",
      "    cur_phrase = [None, []]\n",
      "    verse_phrases = []\n",
      "    \n",
      "    passages_dict = {}\n",
      "    inpassage = False\n",
      "    msg('Look for passages')\n",
      "    for n in NN():\n",
      "        otype = F.otype.v(n)\n",
      "        if otype == 'book':\n",
      "            cur_book = F.book.v(n)\n",
      "        elif otype == 'chapter':\n",
      "            cur_chapter = F.chapter.v(n)\n",
      "        elif otype == 'verse':\n",
      "            cur_verse = F.verse.v(n)\n",
      "            if cur_phrase[0] != None: verse_phrases.append(cur_phrase)\n",
      "            cur_phrase = [None, []]\n",
      "            if verse_phrases:\n",
      "                passages_dict[cur_passage] = {\n",
      "                    'elems': [\n",
      "                        (' '.join([F.surface_consonants.v(y) for y in x[1]]), F.phrase_function.v(x[0])) for x in verse_phrases\n",
      "                    ],\n",
      "                    'text': [\n",
      "                        [(F.text.v(y), F.monads.v(y)) for y in x[1]] for x in verse_phrases\n",
      "                    ],\n",
      "                }\n",
      "            verse_phrases = []\n",
      "            if (cur_book, cur_chapter, cur_verse) in passages: \n",
      "                cur_passage = (cur_book, cur_chapter, cur_verse)\n",
      "                inpassage = True\n",
      "                cur_phrase = [None, []]\n",
      "                verse_phrases = []\n",
      "            else: inpassage = False\n",
      "        elif inpassage:\n",
      "            if otype == 'word': cur_phrase[1].append(n)\n",
      "            elif otype == 'phrase':\n",
      "                if cur_phrase[0] != None: verse_phrases.append(cur_phrase)\n",
      "                cur_phrase = [n, []]\n",
      "    if cur_phrase[0] != None: verse_phrases.append(cur_phrase)\n",
      "    if verse_phrases:\n",
      "        passages_dict[cur_passage] = {\n",
      "            'elems': [\n",
      "                (' '.join([F.surface_consonants.v(y) for y in x[1]]), F.phrase_function.v(x[0])) for x in verse_phrases\n",
      "            ],\n",
      "            'text': [\n",
      "                [(F.text.v(y), F.monads.v(y)) for y in x[1]] for x in verse_phrases\n",
      "            ],\n",
      "        }\n",
      "    msg(\"End walk\")\n",
      "    return passages_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "rules = compile_rules(lines)\n",
      "show = 0\n",
      "rule = rules[show]\n",
      "passage = rule[0]\n",
      "verse = get_passage({passage})[passage]\n",
      "print(\"RULE {}=\\n{}\\nVERSE ELEMS:{}\\nVERSE TEXT={}\".format(show, rule, verse['elems'], verse['text']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    32s Look for passages\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    34s End walk\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RULE 0=\n",
        "[('Exodus', '15', '25'), ('CM', 'Cmpl'), (('CM', 'Loca'), ('FM', 'Pred'), ('LW', 'Cmpl'), ('XQ W MCPV', 'Objc')), 0]\n",
        "VERSE ELEMS:[('W', 'Conj'), ('JY<Q', 'Pred'), ('>L JHWH', 'Cmpl'), ('W', 'Conj'), ('JWRHW', 'PreO'), ('JHWH', 'Subj'), ('<Y', 'Objc'), ('W', 'Conj'), ('JCLK', 'Pred'), ('>L H MJM', 'Cmpl'), ('W', 'Conj'), ('JMTQW', 'Pred'), ('H MJM', 'Subj'), ('CM', 'Loca'), ('FM', 'Pred'), ('LW', 'Cmpl'), ('XQ W MCPV', 'Objc'), ('W', 'Conj'), ('CM', 'Loca'), ('NSHW', 'PreO')]\n",
        "VERSE TEXT=[[('\u05d5\u05b7', '37302')], [('\u05d9\u05b4\u05bc\u05e6\u05b0\u05e2\u05b7\u05a3\u05e7', '37303')], [('\u05d0\u05b6\u05dc', '37304'), ('\u05d9\u05b0\u05d4\u05d5\u05b8\u0597\u05d4', '37305')], [('\u05d5\u05b7', '37306')], [('\u05d9\u05b9\u05bc\u05d5\u05e8\u05b5\u05a4\u05d4\u05d5\u05bc', '37307')], [('\u05d9\u05b0\u05d4\u05d5\u05b8\u05d4\u0599', '37308')], [('\u05e2\u05b5\u0594\u05e5', '37309')], [('\u05d5\u05b7', '37310')], [('\u05d9\u05b7\u05bc\u05e9\u05b0\u05c1\u05dc\u05b5\u05da\u05b0\u0599', '37311')], [('\u05d0\u05b6\u05dc', '37312'), ('\u05d4\u05b7', '37313'), ('\u05de\u05b7\u05bc\u0594\u05d9\u05b4\u05dd', '37314')], [('\u05d5\u05b7\u05bd', '37315')], [('\u05d9\u05b4\u05bc\u05de\u05b0\u05ea\u05b0\u05bc\u05e7\u0596\u05d5\u05bc', '37316')], [('\u05d4\u05b7', '37317'), ('\u05de\u05b8\u05bc\u0591\u05d9\u05b4\u05dd', '37318')], [('\u05e9\u05b8\u05c1\u05a3\u05dd', '37319')], [('\u05e9\u05b8\u05c2\u05a5\u05dd', '37320')], [('\u05dc\u05b9\u059b\u05d5', '37321')], [('\u05d7\u05b9\u05a5\u05e7', '37322'), ('\u05d5\u05bc', '37323'), ('\u05de\u05b4\u05e9\u05b0\u05c1\u05e4\u05b8\u05bc\u0596\u05d8', '37324')], [('\u05d5\u05b0', '37325')], [('\u05e9\u05b8\u05c1\u05a5\u05dd', '37326')], [('\u05e0\u05b4\u05e1\u05b8\u05bc\u05bd\u05d4\u05d5\u05bc', '37327')]]\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Match rules with the database\n",
      "\n",
      "We compare the context part of each rule with the passage we find in the database.\n",
      "We look for all occurrences of the complete context in the elements of the specified passage. \n",
      "\n",
      "The ``match_elem`` function takes care of matching a context against verse material, and after that we run the matching for all rules.\n",
      "\n",
      "The outcome of a match is the list of numbers of the phrases in the verse where the match starts.\n",
      "Particularly interesting are the cases where there is not exactly one match."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def match_elem(context, elements):\n",
      "    matches = []\n",
      "    for (e, elem) in enumerate(elements):\n",
      "        equal = True\n",
      "        for (c, cont) in enumerate(context):\n",
      "            if len(elements) < e + c + 1 or cont[0] != elements[e + c][0]:\n",
      "                equal = False\n",
      "                break\n",
      "        if equal: matches.append(e)\n",
      "    return matches\n",
      "        \n",
      "hits = match_elem(rule[2], elements)\n",
      "hits"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'elements' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-18-4bfa75af5cc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_elem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mhits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'elements' is not defined"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "msg(\"Getting the passages material\")\n",
      "elements_dict = get_passage({r[0] for r in rules})\n",
      "msg(\"Done\")\n",
      "\n",
      "for (r, rule) in enumerate(rules):\n",
      "    (passage, target, context, offset) = rule\n",
      "    verse = elements_dict[passage]\n",
      "    verse_elems = verse['elems']\n",
      "    verse_text = verse['text']\n",
      "    matches = match_elem(context, verse_elems)\n",
      "    nm = len(matches)\n",
      "    print('{}: {} matches: {}'.format(passage, nm, matches))\n",
      "    if nm == 0:\n",
      "        print(\"NO MATCH in RULE {}: '{}' versus {}\\n{}\".format(\n",
      "            r, [t[0] for t in context], [\"{} - {}\".format(t[0][0], ','.join([w[1] for w in t[1]])) for t in zip(verse_elems, verse_text)], lines[r],\n",
      "        ))\n",
      "msg(\"Done\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "16m 48s Getting the passages material\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "16m 48s Look for passages\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "16m 50s End walk\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "16m 50s Done\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "16m 50s Done\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('Exodus', '15', '25'): 1 matches: [13]\n",
        "('Exodus', '40', '8'): 1 matches: [0]\n",
        "('Leviticus', '6', '3'): 1 matches: [16]\n",
        "('Isaiah', '41', '19'): 0 matches: []\n",
        "NO MATCH in RULE 3: '['>FJM', 'B <RBH', 'BRWC TDHR W T>CWR', 'JXDW']' versus ['>TN - 227281', 'B MDBR . - 227282,227284,227283', '>RZ CVH W HDS W <Y CMN - 227285,227286,227287,227288,227289,227290,227291', '>FJM - 227292', 'B <RBH . - 227293,227295,227294', 'BRWC TDHR W T>CWR - 227296,227297,227298,227299', 'JXDW - 227300']\n",
        "JES 41,19\t[>FJM <Pr>]\t[B <RBH <Lo>]\t[BRWC TDHR W T>CWR <Ob>]\t[JXDW <Mo>]\t:CHANGESTO:\t[B <RBH <Co>]\n",
        "\n",
        "('Isaiah', '42', '4'): 0 matches: []\n",
        "NO MATCH in RULE 4: '['<D', 'JFJM', 'B >RY', 'MCPV']' versus ['L> - 227499', 'JKHH - 227500', 'W - 227501', 'L> - 227502', 'JRWY - 227503', '<D - 227504', 'JFJM - 227505', 'B >RY . - 227506,227508,227507', 'MCPV - 227509', 'W - 227510', 'L TWRTW - 227511,227512', '>JJM - 227513', 'JJXJLW - 227514']\n",
        "JES 42,04\t[<D <Cj>]\t[JFJM <Pr>]\t[B >RY <Lo>]\t[MCPV <Ob>]\t:CHANGESTO:\t[B >RY <Co>]\n",
        "\n",
        "('Isaiah', '43', '19'): 0 matches: []\n",
        "NO MATCH in RULE 5: '['>P', '>FJM', 'B MDBR', 'DRK']' versus ['HNNJ - 228150', '<FH - 228151', 'XDCH - 228152', '<TH - 228153', 'TYMX - 228154', 'H - 228155', 'LW> - 228156', 'TD<WH - 228157', '>P - 228158', '>FJM - 228159', 'B MDBR . - 228160,228162,228161', 'DRK - 228163', 'B JCMWN - 228164,228165', 'NHRWT - 228166']\n",
        "JES 43,19\t[>P <Mo>]\t[>FJM <Pr>]\t[B MDBR <Lo>]\t[DRK <Ob>]\t:CHANGESTO:\t[B MDBR <Co>]\n",
        "\n",
        "('Isaiah', '57', '7'): 1 matches: [0]\n",
        "('Isaiah', '57', '8'): 1 matches: [0]\n",
        "('Ezekiel', '20', '28'): 1 matches: [20]\n",
        "('Habakkuk', '2', '9'): 0 matches: []\n",
        "NO MATCH in RULE 9: '['L FWM', 'B MRWM', 'QNW']' versus ['HWJ - 304435', 'BY< - 304436', 'BY< R< - 304437,304438', 'L BJTW - 304439,304440', 'L FWM - 304441,304442', 'B MRWM . - 304443,304445,304444', 'QNW - 304446', 'L HNYL - 304447,304448', 'M KP R< - 304449,304450,304451']\n",
        "HAB 02,09\t[L FWM <Pr>]\t[B MRWM <Lo>]\t[QN +W <Ob>]\t:CHANGESTO:\t[B MRWM <Co>]\n",
        "\n",
        "('I_Samuel', '2', '20'): 1 matches: [6]\n",
        "('II_Samuel', '14', '7'): 0 matches: []\n",
        "NO MATCH in RULE 11: '['L BLTJ FWM', 'L >JCJ', 'CM W C>RJT', '<L PNJ H >DMH']' versus ['W - 168779', 'HNH - 168780', 'QMH - 168781', 'KL H MCPXH - 168782,168783,168784', '<L CPXTK - 168785,168786', 'W - 168787', 'J>MRW - 168788', 'TNJ - 168789', '>T MKH >XJW - 168790,168791,168792', 'W - 168793', 'NMTHW - 168794', 'B NPC >XJW - 168795,168796,168797', '>CR - 168798', 'HRG - 168799', 'W - 168800', 'NCMJDH - 168801', 'GM - 168802', '>T H JWRC - 168803,168804,168805', 'W - 168806', 'KBW - 168807', '>T GXLTJ - 168808,168809', '>CR - 168810', 'NC>RH - 168811', 'L BLTJ - 168812,168813', 'FWM - 168814', 'L >JCJ - 168815,168816', 'CM W C>RJT - 168817,168818,168819', '<L PNJ H >DMH - 168820,168821,168822,168823']\n",
        "IISA 14,07\t[L BLTJ FWM <Pr>]\t[L >JC +J <sc>]\t[CM W C>RJT <Ob>]\t[<L PNJ H >DMH <Co>]\t:CHANGESTO:\t[L >JC +J <Aj>]\n",
        "\n",
        "('I_Kings', '20', '34'): 1 matches: [9]\n",
        "('II_Kings', '4', '10'): 1 matches: [3]\n",
        "('Isaiah', '21', '4'): 1 matches: [4]\n",
        "('I_Samuel', '19', '13'): 1 matches: [7]\n",
        "('II_Kings', '10', '8'): 1 matches: [11]\n",
        "('II_Kings', '13', '7'): 0 matches: []\n",
        "NO MATCH in RULE 17: '['W', 'JFMM', 'K <PR']' versus ['KJ - 204042', 'L> - 204043', 'HC>JR - 204044', 'L JHW>XZ - 204045,204046', '<M - 204047', 'KJ >M - 204048,204049', 'XMCJM PRCJM W <FRH RKB W <FRT >LPJM RGLJ - 204050,204051,204052,204053,204054,204055,204056,204057,204058', 'KJ - 204059', '>BDM - 204060', 'MLK >RM - 204061,204062', 'W - 204063', 'JFMM - 204064', 'K <PR . - 204065,204067,204066', 'L DC - 204068,204069']\n",
        "IIKON13,07\t[W <Cj>]\t[JFM +M <PO>]\t[K <PR <Aj>]\t:CHANGESTO:\t[K <PR <Co>]\n",
        "\n",
        "('Isaiah', '53', '10'): 1 matches: [5]\n",
        "('Jeremiah', '11', '13'): 0 matches: []\n",
        "NO MATCH in RULE 19: '['W', 'MSPR XYWT JRWCLM', 'FMTM', 'MZBXWT', 'L BCT']' versus ['KJ - 241612', 'MSPR <RJK - 241613,241614', 'HJW - 241615', '>LHJK - 241616', 'JHWDH - 241617', 'W - 241618', 'MSPR XYWT JRWCLM - 241619,241620,241621', 'FMTM - 241622', 'MZBXWT - 241623', 'L BCT . - 241624,241626,241625', 'MZBXWT - 241627', 'L QVR - 241628,241629', 'L B<L . - 241630,241632,241631']\n",
        "JER 11,13\t[W <Cj>]\t[MSPR XYWT JRWCLM <Su>]\t[FMTM <Pr>]\t[MZBXWT <Ob>]\t[L BCT <Co>]\t:CHANGESTO:\t[MSPR XYWT JRWCLM <Ob>]\n",
        "\n",
        "('Ezekiel', '17', '5'): 1 matches: [8]\n",
        "('Ezekiel', '19', '5'): 1 matches: [10]\n",
        "('Hosea', '2', '5'): 0 matches: []\n",
        "NO MATCH in RULE 22: '['W', 'FMTJH', 'K MDBR']' versus ['PN - 292357', '>PCJVNH - 292358', '<RMH - 292359', 'W - 292360', 'HYGTJH - 292361', 'K JWM - 292362,292363', 'HWLDH - 292364', 'W - 292365', 'FMTJH - 292366', 'K MDBR . - 292367,292369,292368', 'W - 292370', 'CTH - 292371', 'K >RY YJH - 292372,292373,292374', 'W - 292375', 'HMTJH - 292376', 'B YM> . - 292377,292379,292378']\n",
        "HOS 02,05\t[W <Cj>]\t[FMTJ +H <PO>]\t[K MDBR <Aj>]\t:CHANGESTO:\t[K MDBR <Co>]\n",
        "\n",
        "('Micah', '1', '7'): 1 matches: [7]\n",
        "('Micah', '2', '12'): 0 matches: []\n",
        "NO MATCH in RULE 24: '['JXD', '>FJMNW', 'K Y>N BYRH']' versus ['>SP - 301667', '>>SP - 301668', 'J<QB - 301669', 'KLK - 301670', 'QBY - 301671', '>QBY - 301672', 'C>RJT JFR>L - 301673,301674', 'JXD - 301675', '>FJMNW - 301676', 'K Y>N - 301677,301678', 'BYRH - 301679', 'K <DR - 301680,301681', 'B TWK H DBRW - 301682,301683,301684,301685', 'THJMNH - 301686', 'M >DM - 301687,301688']\n",
        "MICH02,12\t[JXD <Mo>]\t[>FJMN +W <PO>]\t[K Y>N BYRH <Aj>]\t:CHANGESTO:\t[K Y>N BYRH <Co>]\n",
        "\n",
        "('Psalms', '89', '30'): 1 matches: [0]\n",
        "('Job', '38', '9'): 1 matches: [0]\n"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Implementing Flow Charts\n",
      "\n",
      "1. Pick one or two flow charts by Janet.\n",
      "2. translate them in a model.\n",
      "3. Use the model to analyse relevant occurrences of phrases with verbs\n",
      "4. Produce the output of the analysis.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}