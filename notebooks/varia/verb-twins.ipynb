{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img src=\"files/images/laf-fabric-small.png\"/></a>\n",
      "<a href=\"http://www.dans.knaw.nl\" target=\"_blank\"><img src=\"files/images/DANS-small.png\"/></a>\n",
      "<a href=\"http://tla.mpi.nl\" target=\"_blank\"><img src=\"files/images/TLA-small.png\"/></a>\n",
      "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img src=\"files/images/VU-ETCBC-small.png\"/></a>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Twins"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Author**: Thomas Kraan\n",
      "\n",
      "Find verbs in the Hebrew Bible that occur twice or more in a window of so many verbs. Based on a script by Dirk Roorda, modified to the case of verbs instead of words in general and extended with the clause in which the verb occurs as well as location and stem and tense. To this end, first a library of all clauses in the Hebrew Bible is constructed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import collections\n",
      "\n",
      "import laf\n",
      "from laf.fabric import LafFabric\n",
      "fabric = LafFabric()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s This is LAF-Fabric 4.0.2\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fabric.load('bhs3.txt.hdr', '--', 'verb-twins', {\n",
      "    \"primary\": False,\n",
      "    \"xmlids\": {\n",
      "        \"node\": False,\n",
      "        \"edge\": False,\n",
      "    },\n",
      "    \"features\": {\n",
      "        \"shebanq\": {\n",
      "            \"node\": [\n",
      "                \"db.otype\",\n",
      "                \"ft.part_of_speech,lexeme,text,stem,tense,clause_atom_number,word_number_within_book\",\n",
      "                \"sft.verse_label,book\",\n",
      "            ],\n",
      "            \"edge\": [\n",
      "            ],\n",
      "        },\n",
      "    },\n",
      "})\n",
      "exec(Fabric.localnames.format(var='fabric'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s LOADING API: please wait ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.27s LOGFILE=/Users/dirk/laf-fabric-data/etcbc-bhs3/tasks/bhs3.txt.hdr/verb-twins/__log__verb-twins.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.27s INFO: DATA LOADED FROM SOURCE bhs3.txt.hdr AND ANNOX -- FOR TASK verb-twins\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Construct the library of clauses in the Hebrew Bible.\n",
      "It is stored in *all_clauses*. Each entry in *all_clauses*, *clause*, is a dictionary of clauses, corresponding to all clauses of a single book of the Hebrew Bible.\n",
      "A single clause is defined as the set of words that have the same *clause_atom_number* in the LAF dataset of the Hebrew Bible.\n",
      "The single clause is constructed by starting with an empty string and successively adding words until the clause_atom_number changes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_node = {}\n",
      "words = collections.defaultdict(list)\n",
      "all_clauses = {}\n",
      "clause = {}\n",
      "cur_clause_start = \"\"\n",
      "word_clause_atom_number = {}\n",
      "word_word_number_within_book = {}\n",
      "cur_clause_atom_number = None\n",
      "index_book = 0\n",
      "cur_book = None\n",
      "cur_clause_start = \"\"\n",
      "cur_book = None\n",
      "for node in NN():\n",
      "    tmp_book = F.shebanq_sft_book.v(node)\n",
      "    if ((tmp_book != cur_book) and (tmp_book != None)):\n",
      "        # start a new book       \n",
      "        # first store the clauses of the previous books\n",
      "        if (index_book > 0):\n",
      "            all_clauses[index_book] = clause\n",
      "            sys.stderr.write(\"{} {}\\n\".format(len(all_clauses),len(clause)))\n",
      "        cur_book = tmp_book        \n",
      "        index_book += 1        \n",
      "        # start the new book\n",
      "        clause = {}        \n",
      "        word_clause_atom_number = {}\n",
      "        word_word_number_within_book = {}        \n",
      "        cur_clause_atom_number = None        \n",
      "    else:\n",
      "        tmp_clause_atom_number = F.shebanq_ft_clause_atom_number.v(node)\n",
      "        if tmp_clause_atom_number != None:\n",
      "            # start a new clause           \n",
      "            cur_clause_atom_number = tmp_clause_atom_number         \n",
      "            clause[cur_clause_atom_number] = cur_clause_start\n",
      "        else:\n",
      "            # add a new word to the current clause\n",
      "            text = F.shebanq_ft_text.v(node)\n",
      "            if ((text != None) and (cur_clause_atom_number != None)):\n",
      "                clause[cur_clause_atom_number] = \"{} {}\".format(clause[cur_clause_atom_number],text)\n",
      "                # here the word was added.\n",
      "#store the last book\n",
      "all_clauses[index_book] = clause\n",
      "sys.stderr.write(\"{} {}\\n\".format(len(all_clauses),len(clause)))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "1 6087\n",
        "2 4416\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "3 3190\n",
        "4 4043\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "5 3994\n",
        "6 2382\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "7 2881\n",
        "8 4046\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "9 3235\n",
        "10 3682\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "11 3497\n",
        "12 5756\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "13 6294\n",
        "14 5663\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "15 839\n",
        "16 307\n",
        "17 637\n",
        "18 85\n",
        "19 234\n",
        "20 455\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "21 207\n",
        "22 257\n",
        "23 236\n",
        "24 162\n",
        "25 978\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "26 279\n",
        "27 7431\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "28 2954\n",
        "29 2402\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "30 468\n",
        "31 467\n",
        "32 1024\n",
        "33 536\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "34 787\n",
        "35 1826\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "36 860\n",
        "37 1315\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "38 2663\n",
        "39 3486\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Collect for each verb the positions where it occurs.\n",
      "We also need to remember the node for each word position,\n",
      "as well as the exact form of the verb as it occurs in the text, the stem (Qal, Piel etc.) and the tense (perfect, imperfect etc.) \n",
      "\n",
      "The result is *words*, a dictionary that contains for each lexeme a list of its occurrences, in the form of a word number. Similar dictionaries are made for the exact words, their locations, and the stems and the tenses. \n",
      "\n",
      "The code is to a large extent a copy of Dirk Roorda's code, with a test added if the word is a verb (the case we are interested in). Then the lexeme is taken, which is the root (three consonants) of the verb in question. This is exactly what we are interested in. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "verb_node = {}\n",
      "verb_verse = {}\n",
      "verbs = collections.defaultdict(list)\n",
      "verb_stem = {}\n",
      "verb_tense ={}\n",
      "verb_text = {}\n",
      "verb_clause_atom_number = {}\n",
      "all_verb_clause_atom_number = {}\n",
      "verb_word_number_within_book = {}\n",
      "verb_book = {}\n",
      "cur_clause_atom_number = None\n",
      "verb_number = 0\n",
      "cur_verse = None\n",
      "cur_book = None\n",
      "cur_book_number = -1\n",
      "for node in NN():\n",
      "    if F.shebanq_db_otype.v(node) == 'verse':\n",
      "        cur_verse = F.shebanq_sft_verse_label.v(node)        \n",
      "    if F.shebanq_db_otype.v(node) == 'book':\n",
      "        cur_book = F.shebanq_sft_book.v(node)\n",
      "        cur_book_number += 1\n",
      "        #all_word_clause_atom_number[cur_book_number] = word_clause_atom_number\n",
      "    tmp_clause_atom_number = F.shebanq_ft_clause_atom_number.v(node)\n",
      "    if tmp_clause_atom_number != None:\n",
      "        cur_clause_atom_number = tmp_clause_atom_number\n",
      "    part_of_speech = F.shebanq_ft_part_of_speech.v(node)    \n",
      "    if (part_of_speech == 'verb'):\n",
      "        verb_verse[verb_number] = cur_verse\n",
      "        verb = F.shebanq_ft_lexeme.v(node) # the root of the verb\n",
      "        verbs[verb].append(verb_number)\n",
      "        stem = F.shebanq_ft_stem.v(node)\n",
      "        verb_stem[verb_number] = stem   # the stem\n",
      "        tense = F.shebanq_ft_tense.v(node)\n",
      "        verb_tense[verb_number] = tense    # the tense\n",
      "        text = F.shebanq_ft_text.v(node)\n",
      "        verb_text[verb_number] = text   # the verb as it occurs in the text\n",
      "        verb_book[verb_number] = cur_book_number\n",
      "        word_number_within_book = F.shebanq_ft_word_number_within_book.v(node)\n",
      "        verb_word_number_within_book[verb_number] = word_number_within_book       \n",
      "        verb_clause_atom_number[verb_number] = cur_clause_atom_number\n",
      "        # we store the clause number, to be able to look up the context in the\n",
      "        # dictionary of clauses.\n",
      "        verb_node[verb_number] = node\n",
      "        verb_number += 1\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For each verbal root, create buckets of occurrences.\n",
      "If an occurrence occurs within *window_size* positions from the previous occurrence, it goes in the same bucket.\n",
      "If the last occurrence is not that recent, a new bucket is being made.\n",
      "We discard buckets of length 1.\n",
      "This is an exact copy of Dirk Roorda's algorithm (with words replaced by verbs) and explanation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "WINDOW_SIZE = 10\n",
      "bucketset = {}\n",
      "for verb in verbs:\n",
      "    my_bucketset = []\n",
      "    prev_occ = -WINDOW_SIZE - 1\n",
      "    cur_bucket = []\n",
      "    for occ in verbs[verb]:\n",
      "        if occ - prev_occ < WINDOW_SIZE:\n",
      "            cur_bucket.append(occ)\n",
      "        else:\n",
      "            if len(cur_bucket) > 1:\n",
      "                my_bucketset.append(tuple(cur_bucket))\n",
      "            cur_bucket = [occ]\n",
      "        prev_occ = occ\n",
      "    if len(cur_bucket) > 1:\n",
      "        my_bucketset.append(tuple(cur_bucket))\n",
      "    if len(my_bucketset):\n",
      "        bucketset[verb] = tuple(my_bucketset)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now print out the occurrence information in various ways.  Again this is a copy to a large extent of Dirk Roorda's script.\n",
      "\n",
      "**Summary file** for each verbal root: number of buckets, size of longest bucket, the lexeme itself.\n",
      "\n",
      "**Passage file** for each verbal root: a line containing the lexemes, plus lines containing the buckets, represented by their passages.\n",
      "Furthermore, the verb as it occurs in the text, the word number within the book of Psalms, the word atom clause number, the Hebrew text of the clause in which the verb occurs, the stem and the tense. \n",
      "\n",
      "A test for very frequent verbs as it was present in the notebook by Dirk Roorda for very frequent words has been removed.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "summary_file = \"summary_all_books.txt\"\n",
      "passage_file = \"passages_all_books.txt\"\n",
      "sump = outfile(summary_file)\n",
      "result = outfile(passage_file)\n",
      "for (verb, bset) in sorted(bucketset.items(), key=lambda x: (-len(x[1]), -max([len(b) for b in x[1]]), x[0])):\n",
      "    max_len = max([len(b) for b in bset])\n",
      "    if max_len > 2:\n",
      "        sump.write(\"{}\\t{}\\t{}\\n\".format(len(bset), max_len, verb))\n",
      "    result.write(\"{}\\n\".format(verb))\n",
      "    for b in bset:\n",
      "        passages = collections.OrderedDict()\n",
      "        for occ in b:\n",
      "            verse = verb_verse[occ]\n",
      "            if verse not in passages:\n",
      "                passages[verse] = 1\n",
      "            else:\n",
      "                passages[verse] += 1\n",
      "        result.write(\"\\t{}\\n\".format(\", \".join([\"{} x {}\".format(verse, n) for (verse, n) in passages.items()])))\n",
      "        for occ in b:\n",
      "            result.write(\"\\t{}\\n\".format(verb_text[occ]))\n",
      "            result.write(\"\\t{}\\n\".format(verb_word_number_within_book[occ]))\n",
      "            result.write(\"\\t{}\\n\".format(verb_clause_atom_number[occ]))\n",
      "            clause = all_clauses[verb_book[occ]+1]\n",
      "            result.write(\"\\t{}\\n\".format(clause[verb_clause_atom_number[occ]]))\n",
      "            result.write(\"\\t{}\\n\".format(verb_stem[occ]))\n",
      "            result.write(\"\\t{}\\n\".format(verb_tense[occ]))\n",
      "        result.write(\"\\n\\n\")\n",
      "sump.close()\n",
      "result.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "API['close']()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 00s Results directory:\n",
        "/Users/dirk/laf-fabric-data/etcbc-bhs3/tasks/bhs3.txt.hdr/verb-twins\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "__log__verb-twins.txt                   229 Mon Mar 24 17:42:54 2014\n",
        "passages_all_books.txt              4122708 Mon Mar 24 17:42:40 2014\n",
        "summary_all_books.txt                  3539 Mon Mar 24 17:42:40 2014\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
