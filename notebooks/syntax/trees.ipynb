{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img src=\"files/images/laf-fabric-small.png\"/></a>\n",
      "<a href=\"http://www.dans.knaw.nl\" target=\"_blank\"><img src=\"files/images/DANS-small.png\"/></a>\n",
      "<a href=\"http://tla.mpi.nl\" target=\"_blank\"><img src=\"files/images/TLA-small.png\"/></a>\n",
      "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img src=\"files/images/VU-ETCBC-small.png\"/></a>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Trees - the smooth path"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We show the embedding of nodes annotated as sentences, clauses, phrases, subphrases and words.\n",
      "We put them in a format (eventually) such that they can be read by TGREP.\n",
      "Then Rens Bod and Andreas van Cranenburgh can do interesting business with it.\n",
      "\n",
      "Method\n",
      "======\n",
      "\n",
      "We walk through all words and follow them upwards, along parents edges until there are no more outgoing edges.\n",
      "We then have the starting points for our sentences.\n",
      "\n",
      "We walk through the starting points, and for each starting point we assemble the tree hanging off that point.\n",
      "This we do by walking the parents edges in the opposite direction.\n",
      "\n",
      "We use the monad numbers (word numbers) to maintain word order.\n",
      "\n",
      "* parents links from words to phrases to clauses to sentences\n",
      "* word numbers\n",
      "\n",
      "More details will follow below, when we deal with them."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Starting LAF-Fabric"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import collections\n",
      "import laf\n",
      "from laf.fabric import LafFabric\n",
      "from etcbc.preprocess import prepare\n",
      "processor = LafFabric()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s This is LAF-Fabric 4.0.1\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Declaring the features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use the features in our data source for establishing the trees.\n",
      "This is what we need:\n",
      "\n",
      "db.otype\n",
      "--------\n",
      "The type of a node: word, phrase, sentence, etc\n",
      "\n",
      "parents\n",
      "-------\n",
      "This is a feature by which we can identify the edges that correspond to the *parents* relationship.\n",
      "*parents* goes from lower level to higher level (word => ... => sentence).\n",
      "\n",
      "**N.B.** There are two linguistic hierarchies interwoven in this database.\n",
      "Sometimes nodes have more than one parent!\n",
      "\n",
      "ft.text_plain\n",
      "-------------\n",
      "The unvocalized text of a word.\n",
      "\n",
      "ft.part_of_speech\n",
      "-----------------\n",
      "The part of speech of a word.\n",
      "\n",
      "sft.verse-label\n",
      "---------------\n",
      "Passage information: book, chapter, verse all in one feature.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "API = processor.load('bhs3.txt.hdr', '--', 'trees', {\n",
      "    \"xmlids\": {\n",
      "        \"node\": False,\n",
      "        \"edge\": False,\n",
      "    },\n",
      "    \"features\": {\n",
      "        \"shebanq\": {\n",
      "            \"node\": [\n",
      "                \"db.otype,monads\",\n",
      "                \"ft.text_plain,part_of_speech\",\n",
      "                \"ft.clause_constituent_relation,phrase_type\",\n",
      "                \"sft.verse_label\",\n",
      "            ],\n",
      "            \"edge\": [\n",
      "                \"parents.\",\n",
      "            ],\n",
      "        },\n",
      "    },\n",
      "    \"prepare\": prepare,\n",
      "}, verbose='DETAIL')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s LOADING API: please wait ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING m: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING a: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: load main: G.node_anchor_min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.15s DETAIL: load main: G.node_anchor_max\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.28s DETAIL: load main: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.41s DETAIL: load main: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.13s DETAIL: load main: G.edges_from\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.24s DETAIL: load main: G.edges_to\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.36s DETAIL: load main: F.shebanq_db_otype [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.32s DETAIL: load main: F.shebanq_db_monads [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.56s DETAIL: load main: F.shebanq_ft_text_plain [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.98s DETAIL: load main: F.shebanq_ft_part_of_speech [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.28s DETAIL: load main: F.shebanq_ft_clause_constituent_relation [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.34s DETAIL: load main: F.shebanq_ft_phrase_type [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.52s DETAIL: load main: F.shebanq_sft_verse_label [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.54s DETAIL: load main: F.shebanq_parents_ [e] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.91s DETAIL: load main: C.shebanq_parents_ -> \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  6.24s DETAIL: load main: C.shebanq_parents_ <- \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  7.38s LOGFILE=/Users/dirk/laf-fabric-data/etcbc-bhs3/tasks/bhs3.txt.hdr/trees/__log__trees.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  7.38s DETAIL: prep prep: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  7.52s DETAIL: prep prep: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.04s INFO: DATA LOADED FROM SOURCE bhs3.txt.hdr AND ANNOX -- FOR TASK trees\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Configuration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we define the formatting of the trees.\n",
      "\n",
      "Relevant nodes\n",
      "--------------\n",
      "Not all nodes will be shown in the output.\n",
      "The nodes that are shown, have abbreviated names.\n",
      "Nodes with ``True`` will be shown, nodes with ``False`` will be suppressed.\n",
      "\n",
      "Suppressing a node leaves its children in place. Another way of looking at it, is: we replace a node by its children.\n",
      "\n",
      "Exception: when a node is visited twice, the second visit refers to the tree built by the first visit.\n",
      "In that case, we do not suppress the node.\n",
      "\n",
      "**N.B.** It turns out that the ``-atom`` nodes are never visited twice.\n",
      "\n",
      "pos_table\n",
      "---------\n",
      "We abbreviate the part-of-speech tags. \n",
      "We include the pos-info by inserting a unary node right above each word."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "F = API['F']\n",
      "NN = API['NN']\n",
      "C = API['C']\n",
      "Ci = API['Ci']\n",
      "msg = API['msg']\n",
      "outfile = API['outfile']\n",
      "my_file = API['my_file']\n",
      "\n",
      "relevant_nodes = [\n",
      "    (\"word\", '', True),\n",
      "    (\"subphrase\", 'SU', True),\n",
      "    (\"phrase_atom\", 'Pa', False),\n",
      "    (\"phrase\", 'P', True),\n",
      "    (\"clause_atom\", 'Ca', False),\n",
      "    (\"clause\", 'C', True),\n",
      "    (\"sentence_atom\", 'Sa', False),\n",
      "    (\"sentence\", 'S', True),\n",
      "]\n",
      "\n",
      "pos_table = {\n",
      " 'adjective': 'aj',\n",
      " 'adverb': 'av',\n",
      " 'article': 'dt',\n",
      " 'conjunction': 'cj',\n",
      " 'interjection': 'ij',\n",
      " 'interrogative': 'ir',\n",
      " 'negative': 'ng',\n",
      " 'noun': 'n',\n",
      " 'preposition': 'pp',\n",
      " 'pronoun': 'pr',\n",
      " 'verb': 'vb',\n",
      "}\n",
      "\n",
      "select_node = set()\n",
      "select_tag = collections.defaultdict(lambda: None)\n",
      "abbrev_node = collections.defaultdict(lambda: None)\n",
      "\n",
      "for (otype, abb, relevant) in relevant_nodes:\n",
      "    if relevant:\n",
      "        select_node.add(abb)\n",
      "    abbrev_node[otype] = abb if abb != None else otype"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Exploration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Currently it is not clear to mehow to use the *mother* edges for the syntax trees.\n",
      "\n",
      "The *phrases* are a bit undifferentiated, and the clauses to.\n",
      "\n",
      "We could add the *clause_constituent_relation* to the syntax trees, and also the *phrase_type* and *phrase_function*.\n",
      "\n",
      "The *clause_constituent_relation* is explored in the notebook *clause_constituent_relation.ipynb*, and the phrase features in the notebook *phrase_typology*."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Find the top nodes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We walk through the words.\n",
      "From each word we walk along the parent edges until we cannot get further.\n",
      "All end points are top nodes. \n",
      "We put the end nodes in a set.\n",
      "We join all sets of end nodes that we have found above each word.\n",
      "\n",
      "**N.B.** In this way we encounter the top nodes many times, but it does not matter,\n",
      "because we put them all in a set, without duplicates."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "msg(\"Looking for top nodes\")\n",
      "top_nodes = set()\n",
      "top_node_types = collections.defaultdict(lambda: 0)\n",
      "top_nodes = C.shebanq_parents_.endnodes(set(NN(test=F.shebanq_db_otype.v, value='word')))\n",
      "\n",
      "msg(\"Top nodes found: {}\".format(len(top_nodes)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  6.84s Looking for top nodes\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    12s Top nodes found: 71354\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Checking"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us see what the types are of all the top nodes we have found.\n",
      "\n",
      "We would like to see that they are all sentences.\n",
      "\n",
      "And are all sentences top nodes?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "top_node_types = collections.defaultdict(lambda: 0)\n",
      "\n",
      "msg(\"Looking up tags for topnodes\")\n",
      "for node in NN(nodes=top_nodes):\n",
      "    tag = abbrev_node[F.shebanq_db_otype.v(node)]\n",
      "    top_node_types[tag] += 1\n",
      "\n",
      "for (otype, tag, relevant) in relevant_nodes:\n",
      "    if top_node_types[tag]:\n",
      "        msg(\"{:<2} {} x at the top\".format(tag, top_node_types[tag]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    17s Looking up tags for topnodes\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    17s S  71354 x at the top\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "msg(\"Non top nodes of type S ... \")\n",
      "nt = 0\n",
      "for node in NN():\n",
      "    if  F.shebanq_db_otype.v(node) == \"sentence\" and C.shebanq_parents_.e(node):\n",
      "        msg(\"{} \".format(node), newline=False, withtime=False)\n",
      "        nt += 1\n",
      "        break\n",
      "msg(\"Non top nodes of type S found: {}\".format(nt))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    20s Non top nodes of type S ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    21s Non top nodes of type S found: 0\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Serializing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For each top node, we serialize its tree along the inverse parents edges.\n",
      "There are confluences, meaning that sometimes one node has two parents.\n",
      "We detect that, and the second time we reach a node, we output a token that references to the tree we constructed in the first visit to that node.\n",
      "\n",
      "We cannot write the string immediately, because after tree creation we want to renumber referenced trees and word occurrences.\n",
      "\n",
      "Output format\n",
      "-------------\n",
      "We output the trees in the order as their texts occur in the Hebrew bible.\n",
      "The output is a text file, and every line corresponds to a exactly one tree.\n",
      "\n",
      "Every line has three tab-separated fields.\n",
      "\n",
      "* passage label\n",
      "* tree structure with placeholders for the words\n",
      "* word sequence (linking words to place holders). This sequence corresponds to the order as found in the text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trees = outfile(\"trees.txt\")\n",
      "\n",
      "nodes_seen = set()\n",
      "words = []\n",
      "sequential = []\n",
      "\n",
      "def write_tree(node):\n",
      "    \n",
      "    if node in nodes_seen:\n",
      "        return\n",
      "    \n",
      "    nodes_seen.add(node)\n",
      "\n",
      "    otype = F.shebanq_db_otype.v(node)\n",
      "    tag = abbrev_node[otype]\n",
      "    relevant = tag in select_node\n",
      "\n",
      "    if tag == 'C':\n",
      "        crr = F.shebanq_ft_clause_constituent_relation.v(node)\n",
      "        if crr != 'none':\n",
      "            tag = crr\n",
      "    elif tag == 'P':\n",
      "        tag = F.shebanq_ft_phrase_type.v(node)\n",
      "    is_word = otype == 'word'\n",
      "    if is_word:\n",
      "        text = F.shebanq_ft_text_plain.v(node)\n",
      "        pos = pos_table[F.shebanq_ft_part_of_speech.v(node)]\n",
      "        monad = int(F.shebanq_db_monads.v(node))\n",
      "        sequential.append((\"W\", len(words)))\n",
      "        words.append((monad, text, pos))\n",
      "    else:\n",
      "        sequential.append((\"O\" if relevant else \"N\", tag))\n",
      "    \n",
      "    for child in Ci.shebanq_parents_.v(node):\n",
      "        write_tree(child)\n",
      "    \n",
      "    if not is_word:\n",
      "        sequential.append((\"C\" if relevant else \"N\", tag))\n",
      "\n",
      "def do_sequential():\n",
      "    word_perm = {}\n",
      "    new_words = sorted(enumerate(words), key=lambda x: x[1][0])\n",
      "    word_rep = ''\n",
      "    for (nn, (on, (monad, text, pos))) in enumerate(new_words):\n",
      "        word_perm[on] = nn\n",
      "        word_rep += '{} '.format(text)\n",
      "                    \n",
      "    for (code, info) in sequential:\n",
      "        if code == 'O' or code == 'C':\n",
      "            if code == 'O':\n",
      "                trees.write('({}'.format(info))\n",
      "            else:\n",
      "                trees.write(')')\n",
      "        elif code == 'W':\n",
      "            nn = word_perm[info]\n",
      "            pos = words[info][2]\n",
      "            trees.write('({} {})'.format(pos, nn))\n",
      "    \n",
      "    trees.write(\"\\t{}\".format(word_rep))\n",
      "    \n",
      "msg(\"Writing trees ...\")\n",
      "verse_label = ''\n",
      "\n",
      "s = 0\n",
      "chunk = 10000\n",
      "sc = 0\n",
      "\n",
      "#for node in NN(test=lambda n: n in top_nodes or F.shebanq_db_otype.v(n) == 'verse', value=True):\n",
      "# The method in the line above turns out to behopelessly inefficient,\n",
      "# I think the lambda expression is the culprit.\n",
      "\n",
      "msg(\"making nodeset\")\n",
      "the_nodes = set(top_nodes) | set(NN(test=F.shebanq_db_otype.v, value='verse'))\n",
      "msg(\"processing topnodes\")\n",
      "for node in NN(nodes=the_nodes):\n",
      "    otype = F.shebanq_db_otype.v(node)\n",
      "    if  otype == 'verse':\n",
      "        verse_label = F.shebanq_sft_verse_label.v(node)\n",
      "        continue\n",
      "    nodes_seen = set()\n",
      "    sequential = []\n",
      "    words = []\n",
      "    write_tree(node)\n",
      "    do_sequential()\n",
      "    trees.write(\"\\t{}\\n\".format(verse_label))\n",
      "    s += 1\n",
      "    sc += 1\n",
      "    if sc == chunk:\n",
      "        msg(\"{} trees written\".format(s))\n",
      "        sc = 0\n",
      "    \n",
      "msg(\"{} trees written\".format(s))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    49s Writing trees ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    49s making nodeset\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    50s processing topnodes\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    52s 10000 trees written\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    54s 20000 trees written\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    56s 30000 trees written\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    58s 40000 trees written\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 00s 50000 trees written\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 01s 60000 trees written\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 03s 70000 trees written\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 03s 71354 trees written\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "API['close']()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 05s Results directory:\n",
        "/Users/dirk/laf-fabric-data/etcbc-bhs3/tasks/bhs3.txt.hdr/trees\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "__log__trees.txt                        697 Sat Mar 22 18:14:30 2014\n",
        "anomalies.txt                         16798 Sat Mar 22 17:46:46 2014\n",
        "trees.txt                           8381583 Sat Mar 22 18:14:30 2014\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Preview"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are the first lines of the output."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -n 25 {my_file('trees.txt')}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S(C(PP(pp 0)(n 1))(VP(vb 2))(NP(n 3))(PP(SU(pp 8)(dt 9)(n 10))(SU(pp 4)(dt 5)(n 6))(cj 7))))\t\u05d1 \u05e8\u05d0\u05e9\u05c1\u05d9\u05ea \u05d1\u05e8\u05d0 \u05d0\u05dc\u05d4\u05d9\u05dd \u05d0\u05ea \u05d4 \u05e9\u05c1\u05de\u05d9\u05dd \u05d5 \u05d0\u05ea \u05d4 \u05d0\u05e8\u05e5 \t GEN 01,01\r\n",
        "(S(C(NP(dt 1)(n 2))(VP(vb 3))(NP(cj 5)(SU(n 4))(SU(n 6)))(CP(cj 0))))\t\u05d5 \u05d4 \u05d0\u05e8\u05e5 \u05d4\u05d9\u05ea\u05d4 \u05ea\u05d4\u05d5 \u05d5 \u05d1\u05d4\u05d5 \t GEN 01,02\r\n",
        "(S(C(CP(cj 0))(NP(n 1))(PP(SU(n 4))(SU(n 3))(pp 2))))\t\u05d5 \u05d7\u05e9\u05c1\u05da \u05e2\u05dc \u05e4\u05e0\u05d9 \u05ea\u05d4\u05d5\u05dd \t GEN 01,02\r\n",
        "(S(C(VP(vb 3))(PP(SU(dt 6)(n 7))(pp 4)(SU(n 5)))(CP(cj 0))(NP(SU(n 1))(SU(n 2)))))\t\u05d5 \u05e8\u05d5\u05d7 \u05d0\u05dc\u05d4\u05d9\u05dd \u05de\u05e8\u05d7\u05e4\u05ea \u05e2\u05dc \u05e4\u05e0\u05d9 \u05d4 \u05de\u05d9\u05dd \t GEN 01,02\r\n",
        "(S(C(CP(cj 0))(VP(vb 1))(NP(n 2))))\t\u05d5 \u05d9\u05d0\u05de\u05e8 \u05d0\u05dc\u05d4\u05d9\u05dd \t GEN 01,03\r\n",
        "(S(C(VP(vb 0))(NP(n 1))))\t\u05d9\u05d4\u05d9 \u05d0\u05d5\u05e8 \t GEN 01,03\r\n",
        "(S(C(VP(vb 1))(NP(n 2))(CP(cj 0))))\t\u05d5 \u05d9\u05d4\u05d9 \u05d0\u05d5\u05e8 \t GEN 01,03\r\n",
        "(S(C(CP(cj 0))(VP(vb 1))(NP(n 2))(PP(pp 3)(dt 4)(n 5)))(Objc(CP(cj 6))(VP(vb 7))))\t\u05d5 \u05d9\u05e8\u05d0 \u05d0\u05dc\u05d4\u05d9\u05dd \u05d0\u05ea \u05d4 \u05d0\u05d5\u05e8 \u05db\u05d9 \u05d8\u05d5\u05d1 \t GEN 01,04\r\n",
        "(S(C(CP(cj 0))(VP(vb 1))(NP(n 2))(PP(SU(n 3)(dt 4)(n 5))(SU(n 9)(n 7)(dt 8))(cj 6))))\t\u05d5 \u05d9\u05d1\u05d3\u05dc \u05d0\u05dc\u05d4\u05d9\u05dd \u05d1\u05d9\u05df \u05d4 \u05d0\u05d5\u05e8 \u05d5 \u05d1\u05d9\u05df \u05d4 \u05d7\u05e9\u05c1\u05da \t GEN 01,04\r\n",
        "(S(C(NP(n 6))(CP(cj 0))(VP(vb 1))(NP(n 2))(PP(pp 3)(dt 4)(n 5))))\t\u05d5 \u05d9\u05e7\u05e8\u05d0 \u05d0\u05dc\u05d4\u05d9\u05dd \u05dc  \u05d0\u05d5\u05e8 \u05d9\u05d5\u05dd \t GEN 01,05\r\n",
        "(S(C(CP(cj 0))(PP(pp 1)(dt 2)(n 3))(VP(vb 4))(NP(n 5))))\t\u05d5 \u05dc  \u05d7\u05e9\u05c1\u05da \u05e7\u05e8\u05d0 \u05dc\u05d9\u05dc\u05d4 \t GEN 01,05\r\n",
        "(S(C(CP(cj 0))(VP(vb 1))(NP(n 2))))\t\u05d5 \u05d9\u05d4\u05d9 \u05e2\u05e8\u05d1 \t GEN 01,05\r\n",
        "(S(C(CP(cj 0))(VP(vb 1))(NP(n 2))))\t\u05d5 \u05d9\u05d4\u05d9 \u05d1\u05e7\u05e8 \t GEN 01,05\r\n",
        "(S(C(NP(SU(n 0))(SU(n 1)))))\t\u05d9\u05d5\u05dd \u05d0\u05d7\u05d3 \t GEN 01,05\r\n",
        "(S(C(CP(cj 0))(VP(vb 1))(NP(n 2))))\t\u05d5 \u05d9\u05d0\u05de\u05e8 \u05d0\u05dc\u05d4\u05d9\u05dd \t GEN 01,06\r\n",
        "(S(C(NP(n 1))(PP(pp 2)(SU(n 3))(SU(dt 4)(n 5)))(VP(vb 0))))\t\u05d9\u05d4\u05d9 \u05e8\u05e7\u05d9\u05e2 \u05d1 \u05ea\u05d5\u05da \u05d4 \u05de\u05d9\u05dd \t GEN 01,06\r\n",
        "(S(C(CP(cj 0))(VP(vb 1))(VP(vb 2))(PP(n 3)(n 4)(pp 5)(n 6))))\t\u05d5 \u05d9\u05d4\u05d9 \u05de\u05d1\u05d3\u05d9\u05dc \u05d1\u05d9\u05df \u05de\u05d9\u05dd \u05dc \u05de\u05d9\u05dd \t GEN 01,06\r\n",
        "(S(C(NP(n 2))(PP(pp 3)(dt 4)(n 5))(CP(cj 0))(VP(vb 1))))\t\u05d5 \u05d9\u05e2\u05e9\u05c2 \u05d0\u05dc\u05d4\u05d9\u05dd \u05d0\u05ea \u05d4 \u05e8\u05e7\u05d9\u05e2 \t GEN 01,07\r\n",
        "(S(Attr(CP(cj 15))(PP(n 20)(pp 16)(pp 17)(pp 18)(dt 19)))(C(CP(cj 0))(VP(vb 1))(PP(n 4)(n 2)(dt 3))(PP(n 12)(dt 13)(n 14))(CP(cj 11)))(Attr(CP(cj 5))(PP(pp 6)(n 7)(pp 8)(dt 9)(n 10))))\t\u05d5 \u05d9\u05d1\u05d3\u05dc \u05d1\u05d9\u05df \u05d4 \u05de\u05d9\u05dd \u05d0\u05e9\u05c1\u05e8 \u05de \u05ea\u05d7\u05ea \u05dc  \u05e8\u05e7\u05d9\u05e2 \u05d5 \u05d1\u05d9\u05df \u05d4 \u05de\u05d9\u05dd \u05d0\u05e9\u05c1\u05e8 \u05de \u05e2\u05dc \u05dc  \u05e8\u05e7\u05d9\u05e2 \t GEN 01,07\r\n",
        "(S(C(CP(cj 0))(VP(vb 1))(AdvP(av 2))))\t\u05d5 \u05d9\u05d4\u05d9 \u05db\u05df \t GEN 01,07\r\n",
        "(S(C(NP(n 2))(PP(dt 4)(n 5)(pp 3))(NP(n 6))(CP(cj 0))(VP(vb 1))))\t\u05d5 \u05d9\u05e7\u05e8\u05d0 \u05d0\u05dc\u05d4\u05d9\u05dd \u05dc  \u05e8\u05e7\u05d9\u05e2 \u05e9\u05c1\u05de\u05d9\u05dd \t GEN 01,08\r\n",
        "(S(C(CP(cj 0))(VP(vb 1))(NP(n 2))))\t\u05d5 \u05d9\u05d4\u05d9 \u05e2\u05e8\u05d1 \t GEN 01,08\r\n",
        "(S(C(NP(n 2))(CP(cj 0))(VP(vb 1))))\t\u05d5 \u05d9\u05d4\u05d9 \u05d1\u05e7\u05e8 \t GEN 01,08\r\n",
        "(S(C(NP(SU(aj 1))(SU(n 0)))))\t\u05d9\u05d5\u05dd \u05e9\u05c1\u05e0\u05d9 \t GEN 01,08\r\n",
        "(S(C(CP(cj 0))(VP(vb 1))(NP(n 2))))\t\u05d5 \u05d9\u05d0\u05de\u05e8 \u05d0\u05dc\u05d4\u05d9\u05dd \t GEN 01,09\r\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}